{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maciej/anaconda3/envs/tokenizex/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from random import random\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "with open(\"asd.txt\", \"r\") as f:\n",
    "    all_txt = f.read()\n",
    "def get_imput(txt, length=512):\n",
    "    bi = int(random()*(len(txt)-1000))\n",
    "    return txt[bi:bi+length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "#dev not working, need positional encoding at the first step to take in to an account!\n",
    "\n",
    "txt = \"wikipediawolna\"\n",
    "\n",
    "def text_emb_pos(txt, tokenizer):\n",
    "    emb_pos = []\n",
    "\n",
    "    for i in range(1, len(txt)+1):\n",
    "        et = tokenizer.encode(txt[:i])\n",
    "        emb_pos.append(list(zip(et, list(range(len(et))))))\n",
    "\n",
    "    return emb_pos\n",
    "\n",
    "def create_mask(emb_pos):\n",
    "    total_len = len(emb_pos)\n",
    "    res = []\n",
    "    mask = np.zeros([total_len, total_len])\n",
    "    for embt, m in zip(emb_pos, mask):\n",
    "        res.append(embt[-1])\n",
    "        m[len(res)-1] = 1\n",
    "        for ie in embt[:-1]:\n",
    "            for j, je in enumerate(res[:-1]):\n",
    "                if je == ie:\n",
    "                    m[j] = 1\n",
    "                    break\n",
    "    return np.array(res), mask\n",
    "\n",
    "def create_mask_embedding(txt, tokenizer):\n",
    "    emb_pos = text_emb_pos(txt, tokenizer)\n",
    "    res, mask = create_mask(emb_pos)    \n",
    "\n",
    "    return res[:, 0], res[:, 1], mask\n",
    "\n",
    "emb, pos, mask = create_mask_embedding(txt, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_masks(o_mask_size, masks):\n",
    "    iy = 0\n",
    "    ix = 0\n",
    "    o_mask = np.zeros((o_mask_size, o_mask_size))\n",
    "\n",
    "    for m in masks:\n",
    "        ms = len(m)\n",
    "        o_mask[iy:iy+ms, ix:ix+ms] += m\n",
    "        o_mask[iy+ms:, ix:ix+ms] = m[-1]\n",
    "        iy+=ms\n",
    "        ix+=ms\n",
    "    return o_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import cache\n",
    "from time import time\n",
    "\n",
    "@cache\n",
    "def process_token(t, tokenizer):\n",
    "    td = tokenizer.decode(t)\n",
    "    if len(td)==1 or \"ï¿½\" in td:\n",
    "        emb = np.array([t]) \n",
    "        pos = np.zeros((1))\n",
    "        mask = np.ones((1,1))\n",
    "    else:\n",
    "        emb, pos, mask = create_mask_embedding(td, tokenizer)\n",
    "        if len(emb) == 1:\n",
    "            raise Exception(\"Used!\")\n",
    "        \n",
    "    return emb, pos, mask\n",
    "\n",
    "\n",
    "def embedding_mask(txt, tokenizer):\n",
    "    tokens = tokenizer.encode(txt)\n",
    "    partial_masks = []\n",
    "\n",
    "    res_emb = []\n",
    "    res_pos = []\n",
    "    pos_len = 0\n",
    "\n",
    "    for t in tokens:\n",
    "        emb, pos, mask = process_token(t, tokenizer)\n",
    "        res_emb.extend(emb)\n",
    "        res_pos.extend(pos+pos_len)\n",
    "        pos_len += pos[-1] + 1\n",
    "        partial_masks.append(mask)\n",
    "\n",
    "    return np.array(res_emb), res_pos, add_masks(len(res_emb), partial_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 1024\n",
    "\n",
    "def tokenize_letters(txt, tokenizer):\n",
    "    emb = []\n",
    "    for l in txt:\n",
    "        emb.extend(tokenizer.encode(l))\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_my():\n",
    "    emb, pos, mask = embedding_mask(txti, tokenizer)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0009926934726536274\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer\n",
    "st = 0\n",
    "for i in range(C):\n",
    "    txti = tokenizer.decode(tokenizer.encode(get_imput(all_txt, 512)))\n",
    "    t1 = time()\n",
    "    emb, pos, mask = embedding_mask(txti, tokenizer)\n",
    "    st += time()-t1\n",
    "    tok = tokenizer.encode(txti)\n",
    "    lte = emb[mask[-1].astype(bool)]\n",
    "    assert lte.tolist() == tok\n",
    "\n",
    "my = st/C\n",
    "print(my)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.98091147467494e-05\n"
     ]
    }
   ],
   "source": [
    "st = 0\n",
    "txt = []\n",
    "for i in range(C):\n",
    "    txt.append(get_imput(all_txt, 512))\n",
    "t1 = time()\n",
    "txti = tokenizer.batch_encode_plus(txt)\n",
    "st += time()-t1\n",
    "    \n",
    "default_batch = st/C\n",
    "print(default_batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001425035297870636\n"
     ]
    }
   ],
   "source": [
    "st = 0\n",
    "for i in range(C):\n",
    "    txt = get_imput(all_txt, 512)\n",
    "    t1 = time()\n",
    "    txti = tokenizer.encode(txt)\n",
    "    st += time()-t1\n",
    "    \n",
    "default = st/C\n",
    "print(default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_atomizer(tokenizer):\n",
    "    \"\"\"\n",
    "    {\n",
    "        \"token_id\":{\n",
    "            \"embedding\":np.array(int), \n",
    "            \"pe\":np.array(int), \n",
    "            \"mask\":np.array(np.array(union(0, 1)))\n",
    "        }\n",
    "    }\n",
    "    \"\"\"\n",
    "    tokenizex_atomizer = {}\n",
    "    vocab = {v:k for k, v in tokenizer.get_vocab().items()}\n",
    "    all_tok = vocab.keys()\n",
    "\n",
    "    for t in all_tok:\n",
    "        e, p, m = process_token(t, tokenizer)\n",
    "        tokenizex_atomizer[t] = (e, p, m)\n",
    "        \n",
    "        # {\n",
    "        #     \"emb\":  e, \n",
    "        #     \"pe\":   p, \n",
    "        #     \"mask\": m\n",
    "        # }\n",
    "        \n",
    "    return tokenizex_atomizer\n",
    "        \n",
    "tokenizex_atomizer = create_atomizer(tokenizer)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0009675367036834359\n"
     ]
    }
   ],
   "source": [
    "global st\n",
    "st=0\n",
    "\n",
    "def embedding_mask(txt, tokenizer, tokenizex_atomizer):\n",
    "    global st\n",
    "    tokens = tokenizer.encode(txt)\n",
    "    partial_masks = []\n",
    "\n",
    "    res_emb = []\n",
    "    res_pos = []\n",
    "    pos_len = 0\n",
    "\n",
    "    \n",
    "    for t in tokens:\n",
    "        emb, pos, mask = tokenizex_atomizer[t]\n",
    "        res_emb.extend(emb)\n",
    "        res_pos.extend(pos+pos_len)\n",
    "        pos_len += pos[-1] + 1\n",
    "        partial_masks.append(mask)\n",
    "                \n",
    "    \n",
    "    full_mask = add_masks(len(res_emb), partial_masks)\n",
    "    \n",
    "    return np.array(res_emb), res_pos, full_mask\n",
    "\n",
    "\n",
    "for i in range(C):\n",
    "    txti = tokenizer.decode(tokenizer.encode(get_imput(all_txt, 512)))\n",
    "\n",
    "    t1 = time() #dev\n",
    "    emb, pos, mask = embedding_mask(txti, tokenizer, tokenizex_atomizer)\n",
    "    t2 = time()    \n",
    "    st += t2-t1 #dev\n",
    "\n",
    "    tok = tokenizer.encode(txti)\n",
    "    lte = emb[mask[-1].astype(bool)]\n",
    "    assert lte.tolist() == tok\n",
    "\n",
    "my2 = st/C\n",
    "print(my2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my1/default ratio 6.966097430266907\n",
      "my2/default ratio 6.789563073484433\n",
      "my2/default_batch ratio 48.84300565941267\n"
     ]
    }
   ],
   "source": [
    "llm_examples = 128*500000\n",
    "\n",
    "print(f\"my1/default ratio {my/default}\")\n",
    "print(f\"my2/default ratio {my2/default}\")\n",
    "print(f\"my2/default_batch ratio {my2/default_batch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"example text to tokenize and detokenize\"\n",
    "\n",
    "def decode(id_s, tokenizer):\n",
    "    \n",
    "    emb, pos, mask = embedding_mask(txti, tokenizer, tokenizex_atomizer)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb, pos, mask = embedding_mask(txt, tokenizer, tokenizex_atomizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   68,  1069,    64,   321,   696,   489, 20688,   220,   256,\n",
       "         573, 48659,  2420,   220,   256,   284,   220,   256,   284,\n",
       "          74,   365, 11241,    72,   528,  1096,   220,   257,   281,\n",
       "         290,   220,   288,   390,  1062,    78,   482,  2088,  4233,\n",
       "          72,   528,  1096])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenize_letters(txt, tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "randomllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
