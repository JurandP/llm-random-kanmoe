{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maciej/anaconda3/envs/tokenizex/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast, GPT2Tokenizer\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from random import random\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from data_loader import get_imput\n",
    "from embedding_mask import create_mask_embedding, sum_masks, process_token, encode_em\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maciej/anaconda3/envs/tokenizex/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "vocab = tokenizer.get_vocab()\n",
    "vocabi = {v:k for k, v in vocab.items()}\n",
    "SRC = \"asd_pol.txt\"\n",
    "with open(SRC, \"r\") as f:\n",
    "    all_txt = f.read()\n",
    "def get_imput(txt, length=512):\n",
    "    bi = int(random()*(len(txt)-1000))\n",
    "    return txt[bi:bi+length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize_atoms(txt, tokenizer):\n",
    "    atxt = \"\".join(tokenizer.tokenize(txt))\n",
    "    atxt = list(atxt)\n",
    "    eatxt = tokenizer.encode(atxt)\n",
    "    return eatxt\n",
    "\n",
    "c = 0\n",
    "\n",
    "def lazy_past_tokens_gather(atokens, tokenizer): #dev no dobra, może tak być dla tokenów kłopotliwych, ale dla reszty już nie powinno\n",
    "    global c #dev\n",
    "    c+=1 #dev\n",
    "    past_emb_pos = []\n",
    "    for i in range(1, len(atokens)+1):\n",
    "        past_emb_pos.append(atokens[:i])\n",
    "    return past_emb_pos\n",
    "\n",
    "def past_tokens_gather(atokens, tokenizer):\n",
    "    past_emb_pos = []\n",
    "    for i in range(1, len(atokens)+1):\n",
    "        pdec = tokenizer.decode(atokens[:i])\n",
    "        if '�' in pdec:\n",
    "            return lazy_past_tokens_gather(atokens, tokenizer)\n",
    "        penc = tokenizer.encode(pdec)\n",
    "        past_emb_pos.append(penc)\n",
    "    return past_emb_pos\n",
    "\n",
    "def create_emb_pos_mask(emb_pos):\n",
    "    res = []\n",
    "    # mask = np.eye(len(emb_pos)).astype(bool)\n",
    "    mask = np.eye(len(emb_pos))\n",
    "    for embt, m in zip(emb_pos, mask):\n",
    "        res.append(embt[-1])\n",
    "        for ie in embt[:-1]:\n",
    "            is_added = True if res[:-1] == [] else False\n",
    "            for j, je in enumerate(res[:-1]):\n",
    "                if ie == je:\n",
    "                    # m[j] = True\n",
    "                    m[j] = 1\n",
    "                    is_added = True\n",
    "                    break\n",
    "            if not is_added:\n",
    "                raise Exception(\"target len != input len\")\n",
    "    return np.array(res)[:,0], np.array(res)[:,1], mask\n",
    "\n",
    "def tokenizex_encode_full(txt, tokenizer):\n",
    "    tenc = tokenize_atoms(txt, tokenizer)\n",
    "    res = past_tokens_gather(tenc, tokenizer)\n",
    "    g = [list(zip(e, np.arange(len(e)))) for e in res]\n",
    "    e, p, m = create_emb_pos_mask(g)\n",
    "    return e, p, m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizex_encode(txt, tokenizer, atomizer):\n",
    "    tokens = tokenizer.encode(txt)\n",
    "\n",
    "    res_emb = []\n",
    "    res_pos = []\n",
    "    partial_masks = []\n",
    "    pos_len = 0\n",
    "    \n",
    "    for t in tokens:\n",
    "        emb, pos, mask = atomizer[t]\n",
    "        res_emb.extend(emb)\n",
    "        res_pos.extend(pos+pos_len)\n",
    "        pos_len += pos[-1] + 1\n",
    "        partial_masks.append(mask)   \n",
    "    \n",
    "    full_mask = sum_masks(len(res_emb), partial_masks)\n",
    "    return np.array(res_emb), np.array(res_pos), full_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(txt, emb, pos, mask, tokenizer):\n",
    "    dec = tokenizer.decode(emb[mask[-1].astype(bool)])\n",
    "    assert dec == txt\n",
    "\n",
    "    lemb = tokenize_atoms(txt, tokenizer)\n",
    "    assert len(lemb) == len(emb)\n",
    "\n",
    "    comp = pos[mask[-1].astype(bool)]\n",
    "    for a, b in zip(comp.astype(int), list(range(len(comp)))):\n",
    "        assert a == b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizex_w_encode(txt, tokenizer):\n",
    "    res_emb = []\n",
    "    res_pos = []\n",
    "    partial_masks = []\n",
    "    pos_len = 0\n",
    "    \n",
    "    txtw = [\" \" + e for e in txt.split(\" \")]\n",
    "    if len(txtw[0]) == 1:\n",
    "        del txtw[0]\n",
    "    else:\n",
    "        txtw[0] = txtw[0][1:]\n",
    "    \n",
    "    for w in txtw:\n",
    "        emb, pos, mask = tokenizex_encode_full(w, tokenizer)\n",
    "        res_emb.extend(emb)\n",
    "        res_pos.extend(pos+pos_len)\n",
    "        pos_len += pos[-1] + 1\n",
    "        partial_masks.append(mask)   \n",
    "    full_mask = sum_masks(len(res_emb), partial_masks)\n",
    "    return np.array(res_emb), np.array(res_pos), full_mask\n",
    "\n",
    "def tokenizex_t_encode(txt, tokenizer):\n",
    "    res_emb = []\n",
    "    res_pos = []\n",
    "    partial_masks = []\n",
    "    pos_len = 0\n",
    "    \n",
    "    tok = encoder.encode(txt)\n",
    "    \n",
    "    for w in txtw:\n",
    "        emb, pos, mask = tokenizex_encode_full(w, tokenizer)\n",
    "        res_emb.extend(emb)\n",
    "        res_pos.extend(pos+pos_len)\n",
    "        pos_len += pos[-1] + 1\n",
    "        partial_masks.append(mask)   \n",
    "    full_mask = sum_masks(len(res_emb), partial_masks)\n",
    "    return np.array(res_emb), np.array(res_pos), full_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def remove_whitespace_before_dots(text):\n",
    "    # Używamy wyrażenia regularnego, aby zastąpić białe znaki przed kropkami\n",
    "    modified_text = re.sub(r'\\s+\\.', '.', text)\n",
    "    return modified_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \" return np.array(res_emb), np.array(res_pos), full_mask\"\n",
    "txt = \"\"\"  𡨸 𡨸żóąs\"\"\"\n",
    "txt = \"\"\"cial Maurice Rossel reported that conditions there were \"almost normal\".\n",
    "2014 – Under the terms of UN Security C\"\"\"\n",
    "txt = tokenizer.prepare_for_tokenization(txt)[0]\n",
    "txt = remove_whitespace_before_dots(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = errorous\n",
    "txt = \"\"\"\\n\\nY\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[198, 198, 56]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nY'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([628, 56])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[198, 198, 56]\n",
      "[[198], [628], [198, 198, 56]]\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "target len != input len",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(res)\n\u001b[1;32m      5\u001b[0m g \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(e, np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(e)))) \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m res]\n\u001b[0;32m----> 6\u001b[0m e, p, m \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_emb_pos_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(e)\n\u001b[1;32m      8\u001b[0m validate(txt, e, p, m, tokenizer)\n",
      "Cell \u001b[0;32mIn[4], line 42\u001b[0m, in \u001b[0;36mcreate_emb_pos_mask\u001b[0;34m(emb_pos)\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_added:\n\u001b[0;32m---> 42\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget len != input len\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(res)[:,\u001b[38;5;241m0\u001b[39m], np\u001b[38;5;241m.\u001b[39marray(res)[:,\u001b[38;5;241m1\u001b[39m], mask\n",
      "\u001b[0;31mException\u001b[0m: target len != input len"
     ]
    }
   ],
   "source": [
    "tenc = tokenize_atoms(txt, tokenizer)\n",
    "print(tenc)\n",
    "res = past_tokens_gather(tenc, tokenizer)\n",
    "print(res)\n",
    "g = [list(zip(e, np.arange(len(e)))) for e in res]\n",
    "e, p, m = create_emb_pos_mask(g)\n",
    "print(e)\n",
    "validate(txt, e, p, m, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "target len != input len",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[85], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m e, p, m \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizex_encode_full\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtxt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m validate(txt, e, p, m, tokenizer)\n",
      "Cell \u001b[0;32mIn[4], line 49\u001b[0m, in \u001b[0;36mtokenizex_encode_full\u001b[0;34m(txt, tokenizer)\u001b[0m\n\u001b[1;32m     47\u001b[0m res \u001b[38;5;241m=\u001b[39m past_tokens_gather(tenc, tokenizer)\n\u001b[1;32m     48\u001b[0m g \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(e, np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(e)))) \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m res]\n\u001b[0;32m---> 49\u001b[0m e, p, m \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_emb_pos_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m e, p, m\n",
      "Cell \u001b[0;32mIn[4], line 42\u001b[0m, in \u001b[0;36mcreate_emb_pos_mask\u001b[0;34m(emb_pos)\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_added:\n\u001b[0;32m---> 42\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget len != input len\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(res)[:,\u001b[38;5;241m0\u001b[39m], np\u001b[38;5;241m.\u001b[39marray(res)[:,\u001b[38;5;241m1\u001b[39m], mask\n",
      "\u001b[0;31mException\u001b[0m: target len != input len"
     ]
    }
   ],
   "source": [
    "e, p, m = tokenizex_encode_full(txt, tokenizer)\n",
    "validate(txt, e, p, m, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "target len != input len",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ee, pp, mm \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizex_w_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtxt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# print(ee)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# print(mm)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m validate(txt, ee, pp, mm, tokenizer)\n",
      "Cell \u001b[0;32mIn[7], line 14\u001b[0m, in \u001b[0;36mtokenizex_w_encode\u001b[0;34m(txt, tokenizer)\u001b[0m\n\u001b[1;32m     11\u001b[0m     txtw[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m txtw[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m txtw:\n\u001b[0;32m---> 14\u001b[0m     emb, pos, mask \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizex_encode_full\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     res_emb\u001b[38;5;241m.\u001b[39mextend(emb)\n\u001b[1;32m     16\u001b[0m     res_pos\u001b[38;5;241m.\u001b[39mextend(pos\u001b[38;5;241m+\u001b[39mpos_len)\n",
      "Cell \u001b[0;32mIn[4], line 49\u001b[0m, in \u001b[0;36mtokenizex_encode_full\u001b[0;34m(txt, tokenizer)\u001b[0m\n\u001b[1;32m     47\u001b[0m res \u001b[38;5;241m=\u001b[39m past_tokens_gather(tenc, tokenizer)\n\u001b[1;32m     48\u001b[0m g \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(e, np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(e)))) \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m res]\n\u001b[0;32m---> 49\u001b[0m e, p, m \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_emb_pos_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m e, p, m\n",
      "Cell \u001b[0;32mIn[4], line 42\u001b[0m, in \u001b[0;36mcreate_emb_pos_mask\u001b[0;34m(emb_pos)\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_added:\n\u001b[0;32m---> 42\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget len != input len\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(res)[:,\u001b[38;5;241m0\u001b[39m], np\u001b[38;5;241m.\u001b[39marray(res)[:,\u001b[38;5;241m1\u001b[39m], mask\n",
      "\u001b[0;31mException\u001b[0m: target len != input len"
     ]
    }
   ],
   "source": [
    "ee, pp, mm = tokenizex_w_encode(txt, tokenizer)\n",
    "# print(ee)\n",
    "# print(mm)\n",
    "validate(txt, ee, pp, mm, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_def(txt, tokenizer):\n",
    "    return tokenizer.encode(txt), None, None\n",
    "\n",
    "def tokenizer_my1(txt, tokenizer):\n",
    "    tenc = tokenize_atoms(txt, tokenizer)\n",
    "    res = past_tokens_gather(tenc, tokenizer)\n",
    "    g = [list(zip(e, np.arange(len(e)))) for e in res]\n",
    "    e, p, m = create_emb_pos_mask(g)\n",
    "    return e, p, m\n",
    "\n",
    "def tokenizer_my2(txt, tokenizer):\n",
    "    e, p, m= tokenizex_w_encode(txt, tokenizer)\n",
    "    return e, p, m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "errorous = \"\"\n",
    "def time_valid_tokenization(txt_data, tokenizer, def_tokenizer, C, lbl_val = False):\n",
    "    st = 0\n",
    "    global errorous\n",
    "    for i in range(C):\n",
    "        txt = get_imput(txt_data, 112)\n",
    "        # txti = def_tokenizer.decode(def_tokenizer.encode(get_imput(txt_data, 112)))\n",
    "        txt = def_tokenizer.prepare_for_tokenization(txt)[0]\n",
    "        txt = remove_whitespace_before_dots(txt)\n",
    "        t1 = time()\n",
    "        errorous = txt\n",
    "        emb, pos, mask = tokenizer(txt, def_tokenizer)\n",
    "        st += time()-t1\n",
    "\n",
    "        if lbl_val: \n",
    "            print(\"--------------------\")\n",
    "            print(txt)\n",
    "            validate(txt, emb, pos, mask, def_tokenizer)\n",
    "\n",
    "    return st\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.017826080322265625"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_valid_tokenization(all_txt, tokenizer_def, tokenizer, 100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "From today's featured article\n",
      "Cyclone Taylor\n",
      "Cyclone Taylor (June 23, 1884 – June 9, 1979) was a Canadian profes\n",
      "--------------------\n",
      "ie Mays dies at the age of 93.\n",
      "In basketball, the Boston Celtics defeat the Dallas Mavericks to win the NBA Fina\n",
      "--------------------\n",
      "logo Wikiquote\n",
      "Collection of quotations\n",
      "Wikisource logo Wikisource\n",
      "Free-content library\n",
      "Wikispecies logo Wikispe\n",
      "--------------------\n",
      "Recent deaths: James K. IrvingGerhard KlingenbergDaniel Patrick ReillyAnouk AiméeDonald SutherlandMike Brumley\n",
      "N\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "target len != input len",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtime_valid_tokenization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_txt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_my2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 12\u001b[0m, in \u001b[0;36mtime_valid_tokenization\u001b[0;34m(txt_data, tokenizer, def_tokenizer, C, lbl_val)\u001b[0m\n\u001b[1;32m     10\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m     11\u001b[0m errorous \u001b[38;5;241m=\u001b[39m txt\n\u001b[0;32m---> 12\u001b[0m emb, pos, mask \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtxt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdef_tokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m st \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m time()\u001b[38;5;241m-\u001b[39mt1\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lbl_val: \n",
      "Cell \u001b[0;32mIn[13], line 12\u001b[0m, in \u001b[0;36mtokenizer_my2\u001b[0;34m(txt, tokenizer)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenizer_my2\u001b[39m(txt, tokenizer):\n\u001b[0;32m---> 12\u001b[0m     e, p, m\u001b[38;5;241m=\u001b[39m \u001b[43mtokenizex_w_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtxt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m e, p, m\n",
      "Cell \u001b[0;32mIn[7], line 14\u001b[0m, in \u001b[0;36mtokenizex_w_encode\u001b[0;34m(txt, tokenizer)\u001b[0m\n\u001b[1;32m     11\u001b[0m     txtw[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m txtw[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m txtw:\n\u001b[0;32m---> 14\u001b[0m     emb, pos, mask \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizex_encode_full\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     res_emb\u001b[38;5;241m.\u001b[39mextend(emb)\n\u001b[1;32m     16\u001b[0m     res_pos\u001b[38;5;241m.\u001b[39mextend(pos\u001b[38;5;241m+\u001b[39mpos_len)\n",
      "Cell \u001b[0;32mIn[4], line 49\u001b[0m, in \u001b[0;36mtokenizex_encode_full\u001b[0;34m(txt, tokenizer)\u001b[0m\n\u001b[1;32m     47\u001b[0m res \u001b[38;5;241m=\u001b[39m past_tokens_gather(tenc, tokenizer)\n\u001b[1;32m     48\u001b[0m g \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(e, np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(e)))) \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m res]\n\u001b[0;32m---> 49\u001b[0m e, p, m \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_emb_pos_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m e, p, m\n",
      "Cell \u001b[0;32mIn[4], line 42\u001b[0m, in \u001b[0;36mcreate_emb_pos_mask\u001b[0;34m(emb_pos)\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_added:\n\u001b[0;32m---> 42\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget len != input len\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(res)[:,\u001b[38;5;241m0\u001b[39m], np\u001b[38;5;241m.\u001b[39marray(res)[:,\u001b[38;5;241m1\u001b[39m], mask\n",
      "\u001b[0;31mException\u001b[0m: target len != input len"
     ]
    }
   ],
   "source": [
    "time_valid_tokenization(all_txt, tokenizer_my2, tokenizer, 100, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "yclone Taylor (June 23, 1884 – June 9, 1979) was a Canadian professional ice hockey player and civil servant. Bo\n",
      "--------------------\n",
      "the Renfrew Creamery Kings, becoming the highest-paid athlete in the world on a per-game basis. He then played f\n",
      "--------------------\n",
      "on 2118, the last of Syria's declared chemical weapons were shipped out for destruction.\n",
      "Aymer de Valence, 2nd E\n",
      "--------------------\n",
      "rpoint)?... that because Larrabee County was not established, Iowa remains a state with 99 counties?... that Y\n",
      "--------------------\n",
      " nowym numerem @anatomja x @cukiercukier ❤️ · NOWY ROK · Hej! Zapraszamy Cię! Jeżeli uważasz...\n",
      "\n",
      "ZIOMECZKI ZIOM\n",
      "--------------------\n",
      "base\n",
      "Wikinews logo Wikinews\n",
      "Free-content news\n",
      "Wikiquote logo Wikiquote\n",
      "Collection of quotations\n",
      "Wikisource logo \n",
      "--------------------\n",
      "e statementMobile viewWikimedia FoundationPowered by MediaWiki\n",
      "Przejdź do głównej zawartościUłatwienia dostępu –\n",
      "--------------------\n",
      "artościUłatwienia dostępu – pomoc\n",
      "Opinia o ułatwieniach dostępu\n",
      "Google\n",
      "ziomeczki co tak\n",
      "\n",
      "\n",
      "Wszystko\n",
      "Grafika\n",
      "Wideo\n",
      "--------------------\n",
      ", Red Cross official Maurice Rossel reported that conditions there were \"almost normal\".\n",
      "2014 – Under the terms \n",
      "--------------------\n",
      "+ articles\n",
      "Bahasa IndonesiaBahasa Melayu閩南語 / Bân-lâm-gúБългарскиCatalàČeštinaDanskEsperantoEuskaraעבריתՀայերեն한\n",
      "--------------------\n",
      " Foundation, Inc., a non-profit organization.\n",
      "Privacy policyAbout WikipediaDisclaimersContact WikipediaCode of C\n",
      "--------------------\n",
      "isVenus AnadyomeneLockheed Martin F-35 Lightning II\n",
      "ArchiveMore featured pictures\n",
      "Other areas of Wikipedia\n",
      "Commu\n",
      "--------------------\n",
      "oznaj definicję 'ziomki', wymowę, synonimy i gramatykę. Przeglądaj przykłady użycia 'ziomki' w wielkim korpusie \n",
      "--------------------\n",
      "stępu\n",
      "Google\n",
      "ziomeczki co tak\n",
      "\n",
      "\n",
      "Wszystko\n",
      "Grafika\n",
      "Wideo\n",
      "Wiadomości\n",
      "Książki\n",
      "Finanse\n",
      "Sieć\n",
      "Narzędzia\n",
      "\n",
      "YOUNG MULTI - \n",
      "--------------------\n",
      "lcome to Wikipedia,\n",
      "the free encyclopedia that anyone can edit.\n",
      "6,839,769 articles in English\n",
      "From today's featu\n",
      "--------------------\n",
      "ws about Wikipedia and the broader Wikimedia movement.\n",
      "Teahouse – Ask basic questions about using or editing Wik\n",
      "--------------------\n",
      "olunteer editors and hosted by the Wikimedia Foundation, a non-profit organization that also hosts a range of ot\n",
      "--------------------\n",
      "al weapons were shipped out for destruction.\n",
      "Aymer de Valence, 2nd Earl of Pembroke (d. 1324)Len Hutton (b. 1916\n",
      "--------------------\n",
      "ci\n",
      "16 wrz 2021 — Tak samo jak klip, który w ciągu niecałych dwóch dni zgarnął już ponad 350 tysięcy wyświetleń w\n",
      "--------------------\n",
      "e\n",
      "Przejdź do głównej zawartościUłatwienia dostępu – pomoc\n",
      "Opinia o ułatwieniach dostępu\n",
      "Google\n",
      "ziomeczki co tak\n",
      "\n",
      "--------------------\n",
      "... that at the 2014 Olympic Games, Yulia Lipnitskaya (pictured) became Russia's youngest-ever Winter Olympic g\n",
      "--------------------\n",
      "ੀShqipSlovenčinaSlovenščinaไทยతెలుగుاردو\n",
      "\n",
      "49 languages\n",
      "Text is available under the Creative Commons Attribution-\n",
      "--------------------\n",
      " civil servant. Born and raised in Southern Ontario, Taylor moved to Houghton, Michigan, and played in the Inter\n",
      "--------------------\n",
      "eady high for 932 million people?... that the SkyBridge Alternatives Conference has been called the \"Super Bowl\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "target len != input len",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtime_valid_tokenization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_txt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_my1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 12\u001b[0m, in \u001b[0;36mtime_valid_tokenization\u001b[0;34m(txt_data, tokenizer, def_tokenizer, C, lbl_val)\u001b[0m\n\u001b[1;32m     10\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m     11\u001b[0m errorous \u001b[38;5;241m=\u001b[39m txt\n\u001b[0;32m---> 12\u001b[0m emb, pos, mask \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtxt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdef_tokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m st \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m time()\u001b[38;5;241m-\u001b[39mt1\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lbl_val: \n",
      "Cell \u001b[0;32mIn[13], line 8\u001b[0m, in \u001b[0;36mtokenizer_my1\u001b[0;34m(txt, tokenizer)\u001b[0m\n\u001b[1;32m      6\u001b[0m res \u001b[38;5;241m=\u001b[39m past_tokens_gather(tenc, tokenizer)\n\u001b[1;32m      7\u001b[0m g \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(e, np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(e)))) \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m res]\n\u001b[0;32m----> 8\u001b[0m e, p, m \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_emb_pos_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m e, p, m\n",
      "Cell \u001b[0;32mIn[4], line 42\u001b[0m, in \u001b[0;36mcreate_emb_pos_mask\u001b[0;34m(emb_pos)\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_added:\n\u001b[0;32m---> 42\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget len != input len\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(res)[:,\u001b[38;5;241m0\u001b[39m], np\u001b[38;5;241m.\u001b[39marray(res)[:,\u001b[38;5;241m1\u001b[39m], mask\n",
      "\u001b[0;31mException\u001b[0m: target len != input len"
     ]
    }
   ],
   "source": [
    "time_valid_tokenization(all_txt, tokenizer_my1, tokenizer, 100, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([171, 123, 121]),\n",
       " array([0, 1, 2]),\n",
       " array([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atomizer[120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m txt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m ósiem\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      2\u001b[0m emb, pos, mask \u001b[38;5;241m=\u001b[39m encode_em(txt, tokenizer)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtxt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m, in \u001b[0;36mvalidate\u001b[0;34m(txt, emb, pos, mask, tokenizer)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m dec \u001b[38;5;241m==\u001b[39m txt\n\u001b[1;32m      5\u001b[0m lemb \u001b[38;5;241m=\u001b[39m tokenize_atoms(txt, tokenizer)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lemb) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(emb)\n\u001b[1;32m      8\u001b[0m comp \u001b[38;5;241m=\u001b[39m pos[mask[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mbool\u001b[39m)]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a, b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(comp\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m), \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(comp)))):\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "txt = \"\"\" ósiem\"\"\"\n",
    "emb, pos, mask = encode_em(txt, tokenizer)\n",
    "validate(txt, emb, pos, mask, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \" ósiem\"\n",
    "\n",
    "tokens = tokenizer.tokenize(txt)\n",
    "res_emb = []\n",
    "res_pos = []\n",
    "partial_masks = []\n",
    "pos_len = 0\n",
    "\n",
    "for t in tokens:\n",
    "    emb, pos, mask = process_token(t, tokenizer)\n",
    "\n",
    "    res_emb.extend(emb)\n",
    "    res_pos.extend(pos+pos_len)\n",
    "    pos_len += pos[-1] + 1\n",
    "    partial_masks.append(mask)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1  = \"elo źle żal ósemka no i śląska\"\n",
    "t3 = \"example text to tokenize and, żargon ołówka źle wpływa detokenize, ślóza.\"\n",
    "t1  = \"elo źle żaló\"\n",
    "t3 = \"example.\"\n",
    "e2  = np.array([69])\n",
    "p2  = np.array([0])\n",
    "m2  = np.array([1])\n",
    "t2 = tokenizer.decode(e2)\n",
    "\n",
    "e1, p1, m1 = encode_em(t1, tokenizer)\n",
    "e3, p3, m3 = encode_em(t3, tokenizer)\n",
    "txt = t1+t2+t3\n",
    "txt\n",
    "\n",
    "emb = np.concatenate([e1,e2,e3])\n",
    "pos = sum_pos([p1,p2,p3])\n",
    "mlen = m1.shape[0]+m2.shape[0]+m3.shape[0]\n",
    "mask = sum_masks(mlen, [m1, m2, m3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faulty examples\n",
    "' ósiem'\n",
    "\"ó\"\n",
    "\"\"\" ›\"\"\"\n",
    "''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def text_emb_pos(txt, tokenizer):\n",
    "    emb_pos = []\n",
    "\n",
    "    letter_split = False\n",
    "    for l in txt:\n",
    "        t = tokenizer.encode(l)\n",
    "        if len(t) > 1:\n",
    "            letter_split = True\n",
    "    \n",
    "    if letter_split:\n",
    "        for l in txt:\n",
    "            et = tokenizer.encode(l)\n",
    "            emb_pos.append(list(zip(et, list(range(len(et))))))\n",
    "    else:\n",
    "        for i in range(1, len(txt)+1):\n",
    "            et = tokenizer.encode(txt[:i])\n",
    "            emb_pos.append(list(zip(et, list(range(len(et))))))\n",
    "\n",
    "    return emb_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in atomizer.keys():\n",
    "    txt = tokenizer.decode(t)\n",
    "    emb = tokenize_letters(txt, tokenizer)\n",
    "    a_emb = atomizer[t][0]\n",
    "    assert len(emb) == len(a_emb)\n",
    "    if len(emb) != len(a_emb):\n",
    "        print(len(emb), len(a_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_letters(txt, tokenizer):\n",
    "    emb = []\n",
    "    for l in txt:\n",
    "        emb.extend(tokenizer.encode(l))\n",
    "    return emb\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "randomllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
