{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maciej/anaconda3/envs/tokenizex/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast, GPT2Tokenizer\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from random import random\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from data_loader import get_imput\n",
    "from embedding_mask import create_mask_embedding, sum_masks, process_token, encode_em\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maciej/anaconda3/envs/tokenizex/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "vocab = tokenizer.get_vocab()\n",
    "vocabi = {v:k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize_atoms(txt, tokenizer):\n",
    "    atxt = \"\".join(tokenizer.tokenize(txt))\n",
    "    atxt = list(atxt)\n",
    "    eatxt = tokenizer.encode(atxt)\n",
    "    return eatxt\n",
    "\n",
    "def past_tokens_gather(atokens, tokenizer):\n",
    "    past_emb_pos = []\n",
    "    for i in range(1, len(atokens)+1):\n",
    "        partial = atokens[:i]\n",
    "        pdec = tokenizer.decode(partial)\n",
    "        if '�' in pdec:\n",
    "            continue\n",
    "        else:\n",
    "            penc = tokenizer.encode(pdec)\n",
    "        past_emb_pos.append(penc)\n",
    "    return past_emb_pos\n",
    "\n",
    "def create_emb_pos_mask(emb_pos):\n",
    "    res = []\n",
    "    mask = np.eye(len(emb_pos)).astype(bool)\n",
    "    for embt, m in zip(emb_pos, mask):\n",
    "        res.append(embt[-1])\n",
    "        for ie in embt[:-1]:\n",
    "            is_added = True if res[:-1] == [] else False\n",
    "            for j, je in enumerate(res[:-1]):\n",
    "                if ie == je:\n",
    "                    m[j] = True\n",
    "                    is_added = True\n",
    "                    break\n",
    "            if not is_added:\n",
    "                raise Exception(\"target len != input len\")\n",
    "    return np.array(res)[:,0], np.array(res)[:,1], mask\n",
    "\n",
    "def tokenizex_encode(txt, tokenizer):\n",
    "    tenc = tokenize_atoms(txt, tokenizer)\n",
    "    res = past_tokens_gather(tenc, tokenizer)\n",
    "    g = [list(zip(e, np.arange(len(e)))) for e in res]\n",
    "    e, p, m = create_emb_pos_mask(g)\n",
    "    return e, p, m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \" ż\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[220, 129, 120]\n",
      "partial: [220]\n",
      "pdec: ; ;\n",
      "penc: [220]\n",
      "\n",
      "partial: [220, 129, 120]\n",
      "pdec: ; ż;\n",
      "penc: [25370, 120]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tenc = tokenize_atoms(txt, tokenizer)\n",
    "print(tenc)\n",
    "for i in range(1, len(tenc)+1):\n",
    "    partial = tenc[:i]\n",
    "    pdec = tokenizer.decode(partial)\n",
    "    if '�' in pdec:\n",
    "        continue\n",
    "    else:\n",
    "        print(f\"partial: {partial}\")\n",
    "        print(f\"pdec: ;{pdec};\")\n",
    "        penc = tokenizer.encode(pdec)\n",
    "    print(f\"penc: {penc}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ż'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([25370, 120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "target len == input len",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m res \u001b[38;5;241m=\u001b[39m past_tokens_gather(tenc, tokenizer)\n\u001b[1;32m      3\u001b[0m g \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(e, np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(e)))) \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m res]\n\u001b[0;32m----> 4\u001b[0m e, p, m \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_emb_pos_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[32], line 32\u001b[0m, in \u001b[0;36mcreate_emb_pos_mask\u001b[0;34m(emb_pos)\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_added:\n\u001b[0;32m---> 32\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget len == input len\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(res)[:,\u001b[38;5;241m0\u001b[39m], np\u001b[38;5;241m.\u001b[39marray(res)[:,\u001b[38;5;241m1\u001b[39m], mask\n",
      "\u001b[0;31mException\u001b[0m: target len == input len"
     ]
    }
   ],
   "source": [
    "tenc = tokenize_atoms(txt, tokenizer)\n",
    "res = past_tokens_gather(tenc, tokenizer)\n",
    "g = [list(zip(e, np.arange(len(e)))) for e in res]\n",
    "e, p, m = create_emb_pos_mask(g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m e, p, m \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizex_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtxt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mdecode(e[m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\n",
      "Cell \u001b[0;32mIn[4], line 35\u001b[0m, in \u001b[0;36mtokenizex_encode\u001b[0;34m(txt, tokenizer)\u001b[0m\n\u001b[1;32m     33\u001b[0m res \u001b[38;5;241m=\u001b[39m past_tokens_gather(tenc, tokenizer)\n\u001b[1;32m     34\u001b[0m g \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(e, np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(e)))) \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m res]\n\u001b[0;32m---> 35\u001b[0m e, p, m \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_emb_pos_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m e, p, m\n",
      "Cell \u001b[0;32mIn[4], line 28\u001b[0m, in \u001b[0;36mcreate_emb_pos_mask\u001b[0;34m(emb_pos)\u001b[0m\n\u001b[1;32m     26\u001b[0m                 is_added \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     27\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m is_added \u001b[38;5;66;03m#dev\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(res)[:,\u001b[38;5;241m0\u001b[39m], np\u001b[38;5;241m.\u001b[39marray(res)[:,\u001b[38;5;241m1\u001b[39m], mask\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "e, p, m = tokenizex_encode(txt, tokenizer)\n",
    "tokenizer.decode(e[m[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[220, 127, 111]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "at = tokenize_atoms(txt, tokenizer)\n",
    "at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[220], [20543], [6184, 111]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "past_tokens_gather(at, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ĠÃ', '³']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\" ó\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ó'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([220, 127, 111])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'res' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mres\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'res' is not defined"
     ]
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for r in m:\n",
    "    l.append(e[r.astype(bool)].tolist())\n",
    "l == res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def time_valid_tokenization(txt_data, tokenizer, def_tokenizer, C, lbl_val = False):\n",
    "    st = 0\n",
    "    for i in range(C):\n",
    "        txti = def_tokenizer.decode(def_tokenizer.encode(get_imput(txt_data, 512)))\n",
    "        t1 = time()\n",
    "        emb, pos, mask = tokenizer(txti)\n",
    "        st += time()-t1\n",
    "\n",
    "        if lbl_val: \n",
    "            lemb = tokenize_atoms(txti, def_tokenizer)\n",
    "            if len(lemb) != len(emb):\n",
    "                print(txti)\n",
    "            assert len(lemb) == len(emb)\n",
    "\n",
    "            comp = pos[mask[-1].astype(bool)]\n",
    "            for a, b in zip(comp.astype(int), list(range(len(comp)))):\n",
    "                assert a == b\n",
    "\n",
    "    return st\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(txt, emb, pos, mask, tokenizer):\n",
    "    dec = tokenizer.decode(emb[mask[-1].astype(bool)])\n",
    "    assert dec == txt\n",
    "\n",
    "    lemb = tokenize_atoms(txt, tokenizer)\n",
    "    assert len(lemb) == len(emb)\n",
    "\n",
    "    comp = pos[mask[-1].astype(bool)]\n",
    "    for a, b in zip(comp.astype(int), list(range(len(comp)))):\n",
    "        assert a == b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"\"\" ósiem\"\"\"\n",
    "emb, pos, mask = encode_em(txt, tokenizer)\n",
    "validate(txt, emb, pos, mask, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \" ósiem\"\n",
    "\n",
    "tokens = tokenizer.tokenize(txt)\n",
    "res_emb = []\n",
    "res_pos = []\n",
    "partial_masks = []\n",
    "pos_len = 0\n",
    "\n",
    "for t in tokens:\n",
    "    emb, pos, mask = process_token(t, tokenizer)\n",
    "\n",
    "    res_emb.extend(emb)\n",
    "    res_pos.extend(pos+pos_len)\n",
    "    pos_len += pos[-1] + 1\n",
    "    partial_masks.append(mask)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1  = \"elo źle żal ósemka no i śląska\"\n",
    "t3 = \"example text to tokenize and, żargon ołówka źle wpływa detokenize, ślóza.\"\n",
    "t1  = \"elo źle żaló\"\n",
    "t3 = \"example.\"\n",
    "e2  = np.array([69])\n",
    "p2  = np.array([0])\n",
    "m2  = np.array([1])\n",
    "t2 = tokenizer.decode(e2)\n",
    "\n",
    "e1, p1, m1 = encode_em(t1, tokenizer)\n",
    "e3, p3, m3 = encode_em(t3, tokenizer)\n",
    "txt = t1+t2+t3\n",
    "txt\n",
    "\n",
    "emb = np.concatenate([e1,e2,e3])\n",
    "pos = sum_pos([p1,p2,p3])\n",
    "mlen = m1.shape[0]+m2.shape[0]+m3.shape[0]\n",
    "mask = sum_masks(mlen, [m1, m2, m3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faulty examples\n",
    "' ósiem'\n",
    "\"ó\"\n",
    "\"\"\" ›\"\"\"\n",
    "''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def text_emb_pos(txt, tokenizer):\n",
    "    emb_pos = []\n",
    "\n",
    "    letter_split = False\n",
    "    for l in txt:\n",
    "        t = tokenizer.encode(l)\n",
    "        if len(t) > 1:\n",
    "            letter_split = True\n",
    "    \n",
    "    if letter_split:\n",
    "        for l in txt:\n",
    "            et = tokenizer.encode(l)\n",
    "            emb_pos.append(list(zip(et, list(range(len(et))))))\n",
    "    else:\n",
    "        for i in range(1, len(txt)+1):\n",
    "            et = tokenizer.encode(txt[:i])\n",
    "            emb_pos.append(list(zip(et, list(range(len(et))))))\n",
    "\n",
    "    return emb_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in atomizer.keys():\n",
    "    txt = tokenizer.decode(t)\n",
    "    emb = tokenize_letters(txt, tokenizer)\n",
    "    a_emb = atomizer[t][0]\n",
    "    assert len(emb) == len(a_emb)\n",
    "    if len(emb) != len(a_emb):\n",
    "        print(len(emb), len(a_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_letters(txt, tokenizer):\n",
    "    emb = []\n",
    "    for l in txt:\n",
    "        emb.extend(tokenizer.encode(l))\n",
    "    return emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_atomizer(tokenizer):\n",
    "    \"\"\"\n",
    "    {\n",
    "        \"token_id\":{\n",
    "            \"embedding\":np.array(int), \n",
    "            \"pe\":np.array(int), \n",
    "            \"mask\":np.array(np.array(union(0, 1)))\n",
    "        }\n",
    "    }\n",
    "    \"\"\"\n",
    "    tokenizex_atomizer = {}\n",
    "    vocab = {v:k for k, v in tokenizer.get_vocab().items()}\n",
    "    all_tok = vocab.keys()\n",
    "\n",
    "    for t in all_tok:\n",
    "        e, p, m = process_token(t, tokenizer)\n",
    "        tokenizex_atomizer[t] = (e, p, m)\n",
    "        \n",
    "    return tokenizex_atomizer\n",
    "        \n",
    "def embedding_mask_atomizer(txt, tokenizer, tokenizex_atomizer):\n",
    "    tokens = tokenizer.encode(txt)\n",
    "    partial_masks = []\n",
    "\n",
    "    res_emb = []\n",
    "    res_pos = []\n",
    "    pos_len = 0\n",
    "    \n",
    "    for t in tokens:\n",
    "        emb, pos, mask = tokenizex_atomizer[t]\n",
    "        res_emb.extend(emb)\n",
    "        res_pos.extend(pos+pos_len)\n",
    "        pos_len += pos[-1] + 1\n",
    "        partial_masks.append(mask)   \n",
    "    \n",
    "    full_mask = sum_masks(len(res_emb), partial_masks)\n",
    "    return np.array(res_emb), np.array(res_pos), full_mask\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "randomllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
