{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast, GPT2Tokenizer\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from random import random\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from embedding_mask import sum_masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "vocab = tokenizer.get_vocab()\n",
    "vocabi = {v:k for k, v in vocab.items()}\n",
    "SRC = \"asd_pol.txt\"\n",
    "with open(SRC, \"r\") as f:\n",
    "    all_txt = f.read()\n",
    "def get_imput(txt, length=512):\n",
    "    bi = int(random()*(len(txt)-1000))\n",
    "    return txt[bi:bi+length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from functools import cache\n",
    "\n",
    "\n",
    "def tokenize_atoms(txt, tokenizer):\n",
    "    atxt = \"\".join(tokenizer.tokenize(txt))\n",
    "    atxt = list(atxt)\n",
    "    eatxt = tokenizer.encode(atxt)\n",
    "    return eatxt\n",
    "\n",
    "c = 0\n",
    "\n",
    "def lazy_past_tokens_gather(atokens, tokenizer): #dev no dobra, może tak być dla tokenów kłopotliwych, ale dla reszty już nie powinno\n",
    "    global c #dev\n",
    "    c+=1 #dev\n",
    "    past_emb_pos = []\n",
    "    for i in range(1, len(atokens)+1):\n",
    "        past_emb_pos.append(atokens[:i])\n",
    "    return past_emb_pos\n",
    "\n",
    "def past_tokens_gather(atokens, tokenizer):\n",
    "    past_emb_pos = []\n",
    "    for i in range(1, len(atokens)+1):\n",
    "        pdec = tokenizer.decode(atokens[:i])\n",
    "        if '�' in pdec:\n",
    "            return lazy_past_tokens_gather(atokens, tokenizer)\n",
    "        penc = tokenizer.encode(pdec)\n",
    "        past_emb_pos.append(penc)\n",
    "    return past_emb_pos\n",
    "\n",
    "def create_emb_pos_mask(emb_pos):\n",
    "    res = []\n",
    "    # mask = np.eye(len(emb_pos)).astype(bool)\n",
    "    mask = np.eye(len(emb_pos))\n",
    "    for embt, m in zip(emb_pos, mask):\n",
    "        res.append(embt[-1])\n",
    "        for ie in embt[:-1]:\n",
    "            is_added = True if res[:-1] == [] else False\n",
    "            for j, je in enumerate(res[:-1]):\n",
    "                if ie == je:\n",
    "                    m[j] = 1\n",
    "                    is_added = True\n",
    "                    break\n",
    "            if not is_added:\n",
    "                raise Exception(\"target len != input len\")\n",
    "    return np.array(res)[:,0], np.array(res)[:,1], mask\n",
    "\n",
    "# @cache\n",
    "def tokenizex_encode_full(txt, tokenizer):\n",
    "    tenc = tokenize_atoms(txt, tokenizer)\n",
    "    res = past_tokens_gather(tenc, tokenizer)\n",
    "    g = [list(zip(e, np.arange(len(e)))) for e in res]\n",
    "    e, p, m = create_emb_pos_mask(g)\n",
    "    return e, p, m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizex_encode(txt, tokenizer, atomizer):\n",
    "    tokens = tokenizer.encode(txt)\n",
    "\n",
    "    res_emb = []\n",
    "    res_pos = []\n",
    "    partial_masks = []\n",
    "    pos_len = 0\n",
    "    \n",
    "    for t in tokens:\n",
    "        emb, pos, mask = atomizer[t]\n",
    "        res_emb.extend(emb)\n",
    "        res_pos.extend(pos+pos_len)\n",
    "        pos_len += pos[-1] + 1\n",
    "        partial_masks.append(mask)   \n",
    "    \n",
    "    full_mask = sum_masks(len(res_emb), partial_masks)\n",
    "    return np.array(res_emb), np.array(res_pos), full_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def remove_whitespace_before_dots_p(text):\n",
    "    # Używamy wyrażenia regularnego, aby zastąpić białe znaki przed kropkami\n",
    "    modified_text = re.sub(r'\\s+\\.', '.', text)\n",
    "    modified_text = re.sub(r'\\n+\\n', '\\n', modified_text)\n",
    "    return modified_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(txt, emb, pos, mask, tokenizer):\n",
    "    txt = remove_whitespace_before_dots_p(txt)\n",
    "    dec = tokenizer.decode(emb[mask[-1].astype(bool)])\n",
    "    assert dec == txt\n",
    "\n",
    "    lemb = tokenize_atoms(txt, tokenizer)\n",
    "    assert len(lemb) == len(emb)\n",
    "\n",
    "    comp = pos[mask[-1].astype(bool)]\n",
    "    for a, b in zip(comp.astype(int), list(range(len(comp)))):\n",
    "        assert a == b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizex_w_encode(txt, tokenizer):\n",
    "    res_emb = []\n",
    "    res_pos = []\n",
    "    partial_masks = []\n",
    "    pos_len = 0\n",
    "    \n",
    "    txtw = [\" \" + e for e in txt.split(\" \")]\n",
    "    if len(txtw[0]) == 1:\n",
    "        del txtw[0]\n",
    "    else:\n",
    "        txtw[0] = txtw[0][1:]\n",
    "    \n",
    "    for w in txtw:\n",
    "        emb, pos, mask = tokenizex_encode_full(w, tokenizer)\n",
    "        res_emb.extend(emb)\n",
    "        res_pos.extend(pos+pos_len)\n",
    "        pos_len += pos[-1] + 1\n",
    "        partial_masks.append(mask)   \n",
    "    full_mask = sum_masks(len(res_emb), partial_masks)\n",
    "    return np.array(res_emb), np.array(res_pos), full_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cial Maurice Rossel reported that conditions there were \"almost normal\".\\n2014 – Under the terms of UN Security C'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = \" return np.array(res_emb), np.array(res_pos), full_mask\"\n",
    "txt = \"\"\"  𡨸 𡨸żóąs\"\"\"\n",
    "txt = \"\"\"cial Maurice Rossel reported that conditions there were \"almost normal\".\n",
    "2014 – Under the terms of UN Security C\"\"\"\n",
    "txt = tokenizer.prepare_for_tokenization(txt)[0]\n",
    "txt = remove_whitespace_before_dots_p(txt)\n",
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# txt = errorous\n",
    "txt = \"\"\"\\n\\nY\"\"\"\n",
    "txt = '\\n\\nf\\ngh\\n\\n\\n\\nSta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nf\\ngh\\nSta'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r'\\n+\\n', '\\n', txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = remove_whitespace_before_dots_p(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tenc = tokenize_atoms(txt, tokenizer)\n",
    "# print(tenc)\n",
    "res = past_tokens_gather(tenc, tokenizer)\n",
    "# print(res)\n",
    "g = [list(zip(e, np.arange(len(e)))) for e in res]\n",
    "e, p, m = create_emb_pos_mask(g)\n",
    "# print(e)\n",
    "# print(m)\n",
    "validate(txt, e, p, m, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "e, p, m = tokenizex_encode_full(txt, tokenizer)\n",
    "validate(txt, e, p, m, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee, pp, mm = tokenizex_w_encode(txt, tokenizer)\n",
    "# print(ee)\n",
    "# print(mm)\n",
    "validate(txt, ee, pp, mm, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_def(txt, tokenizer):\n",
    "    return tokenizer.encode(txt), None, None\n",
    "\n",
    "def tokenizer_my1(txt, tokenizer):\n",
    "    tenc = tokenize_atoms(txt, tokenizer)\n",
    "    res = past_tokens_gather(tenc, tokenizer)\n",
    "    g = [list(zip(e, np.arange(len(e)))) for e in res]\n",
    "    e, p, m = create_emb_pos_mask(g)\n",
    "    return e, p, m\n",
    "\n",
    "def tokenizer_my2(txt, tokenizer):\n",
    "    e, p, m= tokenizex_w_encode(txt, tokenizer)\n",
    "    return e, p, m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "errorous = \"\"\n",
    "def time_valid_tokenization(txt_data, tokenizer, def_tokenizer, C, lbl_val = False):\n",
    "    st = 0\n",
    "    global errorous\n",
    "    for i in range(C):\n",
    "        txt = get_imput(txt_data, 512)\n",
    "        # txti = def_tokenizer.decode(def_tokenizer.encode(get_imput(txt_data, 112)))\n",
    "        txt = def_tokenizer.prepare_for_tokenization(txt)[0]\n",
    "        txt = remove_whitespace_before_dots_p(txt)\n",
    "        t1 = time()\n",
    "        errorous = txt\n",
    "        emb, pos, mask = tokenizer(txt, def_tokenizer)\n",
    "        st += time()-t1\n",
    "\n",
    "        if lbl_val: \n",
    "            validate(txt, emb, pos, mask, def_tokenizer)\n",
    "\n",
    "    return st\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \" ż ż ... asda ... \"\n",
    "txt = remove_whitespace_before_dots_p(txt)\n",
    "emb, pos, mask = tokenizer_my2(txt, tokenizer)\n",
    "validate(txt, emb, pos, mask, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "default = time_valid_tokenization(all_txt, tokenizer_def, tokenizer, 100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "my2 = time_valid_tokenization(all_txt, tokenizer_my2, tokenizer, 100, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my1 = time_valid_tokenization(all_txt, tokenizer_my1, tokenizer, 500, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my2/default ratio 59.47179786923138\n"
     ]
    }
   ],
   "source": [
    "llm_examples = 128*500000\n",
    "\n",
    "# print(f\"my1/default ratio {my1/default}\")\n",
    "print(f\"my2/default ratio {my2/default}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from research.tokenizex.model.tokenizer import TokenizexTokenizer\n",
    "tokenizex = TokenizexTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \" ż ż ... asda ... \"\n",
    "\n",
    "e, p, m = tokenizex.text_to_ids_pos_mask(txt)\n",
    "\n",
    "validate(txt, e, p, m, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizex_fin(txt, tokenizex:TokenizexTokenizer):\n",
    "    e,p,m = tokenizex.text_to_ids_pos_mask(txt)\n",
    "    tokenizex.validate_tokenization(txt,e,p,m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizex_fin(\"sdfsdfsd\", tokenizex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encode_em' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m m2  \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m      6\u001b[0m t2 \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(e2)\n\u001b[0;32m----> 8\u001b[0m e1, p1, m1 \u001b[38;5;241m=\u001b[39m \u001b[43mencode_em\u001b[49m(t1, tokenizer)\n\u001b[1;32m      9\u001b[0m e3, p3, m3 \u001b[38;5;241m=\u001b[39m encode_em(t3, tokenizer)\n\u001b[1;32m     10\u001b[0m txt \u001b[38;5;241m=\u001b[39m t1\u001b[38;5;241m+\u001b[39mt2\u001b[38;5;241m+\u001b[39mt3\n",
      "\u001b[0;31mNameError\u001b[0m: name 'encode_em' is not defined"
     ]
    }
   ],
   "source": [
    "t3 = \"example text to tokenize and, żargon ołówka źle wpływa detokenize, ślóza.\"\n",
    "t3 = \"example.\"\n",
    "e2  = np.array([69])\n",
    "p2  = np.array([0])\n",
    "m2  = np.array([1])\n",
    "t2 = tokenizer.decode(e2)\n",
    "\n",
    "e1, p1, m1 = encode_em(t1, tokenizer)\n",
    "e3, p3, m3 = encode_em(t3, tokenizer)\n",
    "txt = t1+t2+t3\n",
    "txt\n",
    "\n",
    "emb = np.concatenate([e1,e2,e3])\n",
    "pos = sum_pos([p1,p2,p3])\n",
    "mlen = m1.shape[0]+m2.shape[0]+m3.shape[0]\n",
    "mask = sum_masks(mlen, [m1, m2, m3])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "randomllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
