{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maciej/anaconda3/envs/tokenizex/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/maciej/anaconda3/envs/tokenizex/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from random import random\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "with open(\"asd.txt\", \"r\") as f:\n",
    "    all_txt = f.read()\n",
    "def get_imput(txt, length=512):\n",
    "    bi = int(random()*(len(txt)-1000))\n",
    "    return txt[bi:bi+length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "#dev not working, need positional encoding at the first step to take in to an account!\n",
    "\n",
    "txt = \"wikipediawolna\"\n",
    "\n",
    "def text_emb_pos(txt, tokenizer):\n",
    "    emb_pos = []\n",
    "\n",
    "    for i in range(1, len(txt)+1):\n",
    "        et = tokenizer.encode(txt[:i])\n",
    "        emb_pos.append(list(zip(et, list(range(len(et))))))\n",
    "\n",
    "    return emb_pos\n",
    "\n",
    "def create_mask(emb_pos):\n",
    "    total_len = len(emb_pos)\n",
    "    res = []\n",
    "    mask = np.zeros([total_len, total_len])\n",
    "    for embt, m in zip(emb_pos, mask):\n",
    "        res.append(embt[-1])\n",
    "        m[len(res)-1] = 1\n",
    "        for ie in embt[:-1]:\n",
    "            for j, je in enumerate(res[:-1]):\n",
    "                if je == ie:\n",
    "                    m[j] = 1\n",
    "                    break\n",
    "    return np.array(res), mask\n",
    "\n",
    "def create_mask_embedding(txt, tokenizer):\n",
    "    emb_pos = text_emb_pos(txt, tokenizer)\n",
    "    res, mask = create_mask(emb_pos)    \n",
    "\n",
    "    # return [e[0] for e in res], [e[1] for e in res], mask\n",
    "    return res[:, 0], res[:, 1], mask\n",
    "\n",
    "emb, pos, mask = create_mask_embedding(txt, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unicode_string = u\"Hi!\"\n",
    "type(unicode_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original String: ż\n",
      "UTF-8 String: b'\\xc5\\xbc'\n"
     ]
    }
   ],
   "source": [
    "original_string = \"ż\"\n",
    "utf8_string = original_string.encode('utf-8')\n",
    " \n",
    "print(\"Original String:\", original_string)\n",
    "print(\"UTF-8 String:\", utf8_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokenizer.get_vocab()\n",
    "i_vocab = {v: k for k, v in vocab.items()}\n",
    "tokens = vocab.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_masks(o_mask_size, masks):\n",
    "    iy = 0\n",
    "    ix = 0\n",
    "    o_mask = np.zeros((o_mask_size, o_mask_size))\n",
    "\n",
    "    for m in masks:\n",
    "        ms = len(m)\n",
    "        o_mask[iy:iy+ms, ix:ix+ms] += m\n",
    "        for i in range(iy+ms, o_mask_size):\n",
    "            o_mask[i, ix:ix+ms] += m[-1]\n",
    "        iy+=ms\n",
    "        ix+=ms\n",
    "    return o_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10814678430557251\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "\n",
    "def embedding_mask(txt, tokenizer):\n",
    "    tokens = tokenizer.encode(txt)\n",
    "    partial_masks = []\n",
    "\n",
    "    res_emb = []\n",
    "    res_pos = []\n",
    "    pos_len = 0\n",
    "\n",
    "    for t in tokens:\n",
    "        td = tokenizer.decode(t)\n",
    "        if len(td)==1 or \"�\" in td:\n",
    "            emb = [t] \n",
    "            pos = np.zeros((1))\n",
    "            mask = np.ones((1,1))\n",
    "        else:\n",
    "            emb, pos, mask = create_mask_embedding(td, tokenizer)\n",
    "            if len(emb) == 1:\n",
    "                raise Exception(\"Used!\")\n",
    "        res_emb.extend(emb)\n",
    "        res_pos.extend(pos+pos_len)\n",
    "        pos_len += pos[-1] + 1\n",
    "        partial_masks.append(mask)\n",
    "\n",
    "    return np.array(res_emb), res_pos, add_masks(len(res_emb), partial_masks)\n",
    "\n",
    "txti = \"hey ...\"\n",
    "# txti = \"wikipediawolna\"\n",
    "# txti = \"żółta żółć ...\"\n",
    "\n",
    "txti = tokenizer.decode(tokenizer.encode(txti))\n",
    "\n",
    "\n",
    "C = 100\n",
    "st = 0\n",
    "for i in range(C):\n",
    "    txti = tokenizer.decode(tokenizer.encode(get_imput(all_txt, 1024)))\n",
    "    t1 = time()\n",
    "    emb, pos, mask = embedding_mask(txti, tokenizer)\n",
    "    st += time()-t1\n",
    "    tok = tokenizer.encode(txti)\n",
    "    lte = emb[mask[-1].astype(bool)]\n",
    "    assert lte.tolist() == tok\n",
    "\n",
    "my = st/C\n",
    "print(my)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# lte.tolist() == tok, tok, lte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0002609133720397949\n"
     ]
    }
   ],
   "source": [
    "C = 100\n",
    "st = 0\n",
    "\n",
    "txtil = []\n",
    "for i in range(C):\n",
    "    txt = get_imput(all_txt, 1024)\n",
    "    t1 = time()\n",
    "    txtil = tokenizer.encode(txt)\n",
    "    st += time()-t1\n",
    "    \n",
    "default = st/C\n",
    "print(default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_examples = 128*500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "414.4930689450359"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my/default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0002609133720397949"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.030662436485290528"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "randomllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
