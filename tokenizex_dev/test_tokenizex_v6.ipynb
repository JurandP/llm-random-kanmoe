{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maciej/anaconda3/envs/tokenizex/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast, GPT2Tokenizer\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from random import random\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from data_loader import get_imput\n",
    "from embedding_mask import create_mask_embedding, sum_masks, process_token, encode_em\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maciej/anaconda3/envs/tokenizex/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "vocab = tokenizer.get_vocab()\n",
    "vocabi = {v:k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize_atoms(txt, tokenizer):\n",
    "    atxt = \"\".join(tokenizer.tokenize(txt))\n",
    "    atxt = list(atxt)\n",
    "    eatxt = tokenizer.encode(atxt)\n",
    "    return eatxt\n",
    "\n",
    "c = 0\n",
    "\n",
    "def lazy_past_tokens_gather(atokens, tokenizer): #dev no dobra, może tak być dla tokenów kłopotliwych, ale dla reszty już nie powinno\n",
    "    global c #dev\n",
    "    c+=1 #dev\n",
    "    past_emb_pos = []\n",
    "    for i in range(1, len(atokens)+1):\n",
    "        past_emb_pos.append(atokens[:i])\n",
    "    return past_emb_pos\n",
    "\n",
    "def past_tokens_gather(atokens, tokenizer):\n",
    "    past_emb_pos = []\n",
    "    for i in range(1, len(atokens)+1):\n",
    "        pdec = tokenizer.decode(atokens[:i])\n",
    "        if '�' in pdec:\n",
    "            return lazy_past_tokens_gather(atokens, tokenizer)\n",
    "        penc = tokenizer.encode(pdec)\n",
    "        past_emb_pos.append(penc)\n",
    "    return past_emb_pos\n",
    "\n",
    "\n",
    "def create_emb_pos_mask(emb_pos):\n",
    "    res = []\n",
    "    # mask = np.eye(len(emb_pos)).astype(bool)\n",
    "    mask = np.eye(len(emb_pos))\n",
    "    for embt, m in zip(emb_pos, mask):\n",
    "        res.append(embt[-1])\n",
    "        for ie in embt[:-1]:\n",
    "            is_added = True if res[:-1] == [] else False\n",
    "            for j, je in enumerate(res[:-1]):\n",
    "                if ie == je:\n",
    "                    # m[j] = True\n",
    "                    m[j] = 1\n",
    "                    is_added = True\n",
    "                    break\n",
    "            if not is_added:\n",
    "                raise Exception(\"target len != input len\")\n",
    "    return np.array(res)[:,0], np.array(res)[:,1], mask\n",
    "\n",
    "def tokenizex_encode_full(txt, tokenizer):\n",
    "    tenc = tokenize_atoms(txt, tokenizer)\n",
    "    res = past_tokens_gather(tenc, tokenizer)\n",
    "    g = [list(zip(e, np.arange(len(e)))) for e in res]\n",
    "    e, p, m = create_emb_pos_mask(g)\n",
    "    return e, p, m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_token(t, tokenizer):\n",
    "#     td = tokenizer.decode(t)\n",
    "#     if len(td)==1 or \"�\" in td:\n",
    "#         emb = np.array([t]) \n",
    "#         pos = np.zeros((1))\n",
    "#         mask = np.ones((1,1))\n",
    "#     else:\n",
    "#         emb, pos, mask = tokenizex_encode_full(td, tokenizer)\n",
    "#         if len(emb) == 1:\n",
    "#             raise Exception(\"Used!\")\n",
    "        \n",
    "    # return emb, pos, mask\n",
    "\n",
    "def create_atomizer(tokenizer):\n",
    "    \"\"\"\n",
    "    {\n",
    "        \"token_id\":{\n",
    "            \"embedding\":np.array(int), \n",
    "            \"pe\":np.array(int), \n",
    "            \"mask\":np.array(np.array(union(0, 1)))\n",
    "        }\n",
    "    }\n",
    "\n",
    "    \"\"\"\n",
    "    tokenizex_atomizer = {}\n",
    "    vocab = tokenizer.get_vocab()\n",
    "    vocabi = {v:k for k, v in vocab.items()}\n",
    "\n",
    "    for tok, txt in vocabi.items():\n",
    "        e, p, m = tokenizex_encode_full(tokenizer.decode(tok), tokenizer)\n",
    "        tokenizex_atomizer[tok] = (e, p, m)\n",
    "        \n",
    "    return tokenizex_atomizer\n",
    "        \n",
    "def tokenizex_encode(txt, tokenizer, atomizer):\n",
    "    tokens = tokenizer.encode(txt)\n",
    "\n",
    "    res_emb = []\n",
    "    res_pos = []\n",
    "    partial_masks = []\n",
    "    pos_len = 0\n",
    "    \n",
    "    for t in tokens:\n",
    "        emb, pos, mask = atomizer[t]\n",
    "        res_emb.extend(emb)\n",
    "        res_pos.extend(pos+pos_len)\n",
    "        pos_len += pos[-1] + 1\n",
    "        partial_masks.append(mask)   \n",
    "    \n",
    "    full_mask = sum_masks(len(res_emb), partial_masks)\n",
    "    return np.array(res_emb), np.array(res_pos), full_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "873\n"
     ]
    }
   ],
   "source": [
    "c = 0 \n",
    "atomizer = create_atomizer(tokenizer)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(txt, emb, pos, mask, tokenizer):\n",
    "    dec = tokenizer.decode(emb[mask[-1].astype(bool)])\n",
    "    assert dec == txt\n",
    "\n",
    "    lemb = tokenize_atoms(txt, tokenizer)\n",
    "    assert len(lemb) == len(emb)\n",
    "\n",
    "    comp = pos[mask[-1].astype(bool)]\n",
    "    for a, b in zip(comp.astype(int), list(range(len(comp)))):\n",
    "        assert a == b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m txt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  ż\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m ee, pp, mm \u001b[38;5;241m=\u001b[39m tokenizex_encode(txt, tokenizer, atomizer)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtxt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mee\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[37], line 3\u001b[0m, in \u001b[0;36mvalidate\u001b[0;34m(txt, emb, pos, mask, tokenizer)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate\u001b[39m(txt, emb, pos, mask, tokenizer):\n\u001b[1;32m      2\u001b[0m     dec \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(emb[mask[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mbool\u001b[39m)])\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m dec \u001b[38;5;241m==\u001b[39m txt\n\u001b[1;32m      5\u001b[0m     lemb \u001b[38;5;241m=\u001b[39m tokenize_atoms(txt, tokenizer)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lemb) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(emb)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "txt = \"  ż\"\n",
    "\n",
    "ee, pp, mm = tokenizex_encode(txt, tokenizer, atomizer)\n",
    "\n",
    "validate(txt, ee, pp, mm, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizex_encode_dev(txt, tokenizer, atomizer):\n",
    "    tokens = tokenizer.encode(txt)\n",
    "\n",
    "    res_emb = []\n",
    "    res_pos = []\n",
    "    partial_masks = []\n",
    "    pos_len = 0\n",
    "    \n",
    "    for t in tokens:\n",
    "        emb, pos, mask = atomizer[t]\n",
    "        res_emb.extend(emb)\n",
    "        res_pos.extend(pos+pos_len)\n",
    "        pos_len += pos[-1] + 1\n",
    "        partial_masks.append(mask)   \n",
    "    \n",
    "    full_mask = sum_masks(len(res_emb), partial_masks)\n",
    "    return np.array(res_emb), np.array(res_pos), full_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# txt = \"\"\"  𡨸 𡨸żóąs\"\"\"\n",
    "txt = \"  ż\"\n",
    "\n",
    "tenc = tokenize_atoms(txt, tokenizer)\n",
    "res = past_tokens_gather(tenc, tokenizer)\n",
    "g = [list(zip(e, np.arange(len(e)))) for e in res]\n",
    "e, p, m = create_emb_pos_mask(g)\n",
    "\n",
    "validate(txt, e, p, m, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[220, 25370, 120]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([171, 123, 121]),\n",
       " array([0, 1, 2]),\n",
       " array([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atomizer[120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([220, 220, 129, 120])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  𡨸 𡨸żóąs'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e, p, m = tokenizex_encode_full(txt, tokenizer)\n",
    "tokenizer.decode(e[m[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([   36,  9139,    81,   305, 12331]), array([0, 0, 1, 1, 0]), array([[ True, False, False, False, False],\n",
      "       [False,  True, False, False, False],\n",
      "       [False,  True,  True, False, False],\n",
      "       [False,  True, False,  True, False],\n",
      "       [False, False, False, False,  True]]))\n",
      "Error\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[36, 81, 81, 78, 81]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = 12331\n",
    "print(atomizer[num])\n",
    "d = tokenizer.decode(num)\n",
    "print(d)\n",
    "tokenize_atoms(d, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[220,\n",
       " 220,\n",
       " 172,\n",
       " 94,\n",
       " 101,\n",
       " 116,\n",
       " 220,\n",
       " 172,\n",
       " 94,\n",
       " 101,\n",
       " 116,\n",
       " 129,\n",
       " 120,\n",
       " 127,\n",
       " 111,\n",
       " 128,\n",
       " 227,\n",
       " 82]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "at = tokenize_atoms(txt, tokenizer)\n",
    "at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[220],\n",
       " [220, 220],\n",
       " [220, 220, 172],\n",
       " [220, 220, 172, 94],\n",
       " [220, 220, 172, 94, 101],\n",
       " [220, 220, 172, 94, 101, 116],\n",
       " [220, 220, 172, 94, 101, 116, 220],\n",
       " [220, 220, 172, 94, 101, 116, 220, 172],\n",
       " [220, 220, 172, 94, 101, 116, 220, 172, 94],\n",
       " [220, 220, 172, 94, 101, 116, 220, 172, 94, 101],\n",
       " [220, 220, 172, 94, 101, 116, 220, 172, 94, 101, 116],\n",
       " [220, 220, 172, 94, 101, 116, 220, 172, 94, 101, 116, 129],\n",
       " [220, 220, 172, 94, 101, 116, 220, 172, 94, 101, 116, 129, 120],\n",
       " [220, 220, 172, 94, 101, 116, 220, 172, 94, 101, 116, 129, 120, 127],\n",
       " [220, 220, 172, 94, 101, 116, 220, 172, 94, 101, 116, 129, 120, 127, 111],\n",
       " [220,\n",
       "  220,\n",
       "  172,\n",
       "  94,\n",
       "  101,\n",
       "  116,\n",
       "  220,\n",
       "  172,\n",
       "  94,\n",
       "  101,\n",
       "  116,\n",
       "  129,\n",
       "  120,\n",
       "  127,\n",
       "  111,\n",
       "  128],\n",
       " [220,\n",
       "  220,\n",
       "  172,\n",
       "  94,\n",
       "  101,\n",
       "  116,\n",
       "  220,\n",
       "  172,\n",
       "  94,\n",
       "  101,\n",
       "  116,\n",
       "  129,\n",
       "  120,\n",
       "  127,\n",
       "  111,\n",
       "  128,\n",
       "  227],\n",
       " [220,\n",
       "  220,\n",
       "  172,\n",
       "  94,\n",
       "  101,\n",
       "  116,\n",
       "  220,\n",
       "  172,\n",
       "  94,\n",
       "  101,\n",
       "  116,\n",
       "  129,\n",
       "  120,\n",
       "  127,\n",
       "  111,\n",
       "  128,\n",
       "  227,\n",
       "  82]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "past_tokens_gather(at, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ĠÃ', '³']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\" ó\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ó'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([220, 127, 111])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[220],\n",
       " [220, 220],\n",
       " [220, 220, 172],\n",
       " [220, 220, 172, 94],\n",
       " [220, 220, 172, 94, 101],\n",
       " [220, 220, 172, 94, 101, 116],\n",
       " [220, 220, 172, 94, 101, 116, 220],\n",
       " [220, 220, 172, 94, 101, 116, 220, 172],\n",
       " [220, 220, 172, 94, 101, 116, 220, 172, 94],\n",
       " [220, 220, 172, 94, 101, 116, 220, 172, 94, 101],\n",
       " [220, 220, 172, 94, 101, 116, 220, 172, 94, 101, 116],\n",
       " [220, 220, 172, 94, 101, 116, 220, 172, 94, 101, 116, 129],\n",
       " [220, 220, 172, 94, 101, 116, 220, 172, 94, 101, 116, 129, 120],\n",
       " [220, 220, 172, 94, 101, 116, 220, 172, 94, 101, 116, 129, 120, 127],\n",
       " [220, 220, 172, 94, 101, 116, 220, 172, 94, 101, 116, 129, 120, 127, 111],\n",
       " [220,\n",
       "  220,\n",
       "  172,\n",
       "  94,\n",
       "  101,\n",
       "  116,\n",
       "  220,\n",
       "  172,\n",
       "  94,\n",
       "  101,\n",
       "  116,\n",
       "  129,\n",
       "  120,\n",
       "  127,\n",
       "  111,\n",
       "  128],\n",
       " [220,\n",
       "  220,\n",
       "  172,\n",
       "  94,\n",
       "  101,\n",
       "  116,\n",
       "  220,\n",
       "  172,\n",
       "  94,\n",
       "  101,\n",
       "  116,\n",
       "  129,\n",
       "  120,\n",
       "  127,\n",
       "  111,\n",
       "  128,\n",
       "  227],\n",
       " [220,\n",
       "  220,\n",
       "  172,\n",
       "  94,\n",
       "  101,\n",
       "  116,\n",
       "  220,\n",
       "  172,\n",
       "  94,\n",
       "  101,\n",
       "  116,\n",
       "  129,\n",
       "  120,\n",
       "  127,\n",
       "  111,\n",
       "  128,\n",
       "  227,\n",
       "  82]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = []\n",
    "for r in m:\n",
    "    l.append(e[r.astype(bool)].tolist())\n",
    "l == res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def time_valid_tokenization(txt_data, tokenizer, def_tokenizer, C, lbl_val = False):\n",
    "    st = 0\n",
    "    for i in range(C):\n",
    "        txti = def_tokenizer.decode(def_tokenizer.encode(get_imput(txt_data, 512)))\n",
    "        t1 = time()\n",
    "        emb, pos, mask = tokenizer(txti)\n",
    "        st += time()-t1\n",
    "\n",
    "        if lbl_val: \n",
    "            lemb = tokenize_atoms(txti, def_tokenizer)\n",
    "            if len(lemb) != len(emb):\n",
    "                print(txti)\n",
    "            assert len(lemb) == len(emb)\n",
    "\n",
    "            comp = pos[mask[-1].astype(bool)]\n",
    "            for a, b in zip(comp.astype(int), list(range(len(comp)))):\n",
    "                assert a == b\n",
    "\n",
    "    return st\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m txt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m ósiem\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      2\u001b[0m emb, pos, mask \u001b[38;5;241m=\u001b[39m encode_em(txt, tokenizer)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtxt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m, in \u001b[0;36mvalidate\u001b[0;34m(txt, emb, pos, mask, tokenizer)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m dec \u001b[38;5;241m==\u001b[39m txt\n\u001b[1;32m      5\u001b[0m lemb \u001b[38;5;241m=\u001b[39m tokenize_atoms(txt, tokenizer)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lemb) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(emb)\n\u001b[1;32m      8\u001b[0m comp \u001b[38;5;241m=\u001b[39m pos[mask[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mbool\u001b[39m)]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a, b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(comp\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m), \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(comp)))):\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "txt = \"\"\" ósiem\"\"\"\n",
    "emb, pos, mask = encode_em(txt, tokenizer)\n",
    "validate(txt, emb, pos, mask, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \" ósiem\"\n",
    "\n",
    "tokens = tokenizer.tokenize(txt)\n",
    "res_emb = []\n",
    "res_pos = []\n",
    "partial_masks = []\n",
    "pos_len = 0\n",
    "\n",
    "for t in tokens:\n",
    "    emb, pos, mask = process_token(t, tokenizer)\n",
    "\n",
    "    res_emb.extend(emb)\n",
    "    res_pos.extend(pos+pos_len)\n",
    "    pos_len += pos[-1] + 1\n",
    "    partial_masks.append(mask)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1  = \"elo źle żal ósemka no i śląska\"\n",
    "t3 = \"example text to tokenize and, żargon ołówka źle wpływa detokenize, ślóza.\"\n",
    "t1  = \"elo źle żaló\"\n",
    "t3 = \"example.\"\n",
    "e2  = np.array([69])\n",
    "p2  = np.array([0])\n",
    "m2  = np.array([1])\n",
    "t2 = tokenizer.decode(e2)\n",
    "\n",
    "e1, p1, m1 = encode_em(t1, tokenizer)\n",
    "e3, p3, m3 = encode_em(t3, tokenizer)\n",
    "txt = t1+t2+t3\n",
    "txt\n",
    "\n",
    "emb = np.concatenate([e1,e2,e3])\n",
    "pos = sum_pos([p1,p2,p3])\n",
    "mlen = m1.shape[0]+m2.shape[0]+m3.shape[0]\n",
    "mask = sum_masks(mlen, [m1, m2, m3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faulty examples\n",
    "' ósiem'\n",
    "\"ó\"\n",
    "\"\"\" ›\"\"\"\n",
    "''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def text_emb_pos(txt, tokenizer):\n",
    "    emb_pos = []\n",
    "\n",
    "    letter_split = False\n",
    "    for l in txt:\n",
    "        t = tokenizer.encode(l)\n",
    "        if len(t) > 1:\n",
    "            letter_split = True\n",
    "    \n",
    "    if letter_split:\n",
    "        for l in txt:\n",
    "            et = tokenizer.encode(l)\n",
    "            emb_pos.append(list(zip(et, list(range(len(et))))))\n",
    "    else:\n",
    "        for i in range(1, len(txt)+1):\n",
    "            et = tokenizer.encode(txt[:i])\n",
    "            emb_pos.append(list(zip(et, list(range(len(et))))))\n",
    "\n",
    "    return emb_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in atomizer.keys():\n",
    "    txt = tokenizer.decode(t)\n",
    "    emb = tokenize_letters(txt, tokenizer)\n",
    "    a_emb = atomizer[t][0]\n",
    "    assert len(emb) == len(a_emb)\n",
    "    if len(emb) != len(a_emb):\n",
    "        print(len(emb), len(a_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_letters(txt, tokenizer):\n",
    "    emb = []\n",
    "    for l in txt:\n",
    "        emb.extend(tokenizer.encode(l))\n",
    "    return emb\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "randomllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
