{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast, GPT2Tokenizer\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from random import random\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from data_loader import get_imput\n",
    "from embedding_mask import create_mask_embedding, sum_masks, process_token, encode_em\n",
    "\n",
    "# def process_token(t, tokenizer):\n",
    "#     td = tokenizer.decode(t)\n",
    "#     if len(td)==1 or \"�\" in td:\n",
    "#         if \"�\" in td:\n",
    "#             vocab = tokenizer.get_vocab()\n",
    "#             vocabi = {v:k for k, v in vocab.items()}\n",
    "#             s_lett = vocabi[t]\n",
    "#             emb = []\n",
    "#             for e in s_lett:\n",
    "#                 emb.append(vocab[e])\n",
    "#             pos = np.arange(len(emb))\n",
    "#             mask = np.tril(np.ones(len(emb)))\n",
    "#         else:\n",
    "#             emb = np.array([t]) \n",
    "#             pos = np.zeros((1))\n",
    "#             mask = np.ones((1,1))\n",
    "#     else:\n",
    "#         emb, pos, mask = create_mask_embedding(td, tokenizer)\n",
    "#         if len(emb) == 1:\n",
    "#             raise Exception(\"Used!\")\n",
    "        \n",
    "#     return emb, pos, mask\n",
    "\n",
    "# def encode_em(txt, tokenizer):\n",
    "#     tokens = tokenizer.encode(txt)\n",
    "\n",
    "#     res_emb = []\n",
    "#     res_pos = []\n",
    "#     partial_masks = []\n",
    "#     pos_len = 0\n",
    "\n",
    "#     for t in tokens:\n",
    "#         emb, pos, mask = process_token(t, tokenizer)\n",
    "\n",
    "#         res_emb.extend(emb)\n",
    "#         res_pos.extend(pos+pos_len)\n",
    "#         pos_len += pos[-1] + 1\n",
    "#         partial_masks.append(mask)\n",
    "\n",
    "#     return np.array(res_emb), np.array(res_pos), sum_masks(len(res_emb), partial_masks)\n",
    "\n",
    "def encode_em(txt, tokenizer):\n",
    "    tokens = tokenizer.tokenize(txt)\n",
    "\n",
    "    \n",
    "\n",
    "    res_emb = []\n",
    "    res_pos = []\n",
    "    partial_masks = []\n",
    "    pos_len = 0\n",
    "\n",
    "    for t in tokens:\n",
    "        emb, pos, mask = process_token(t, tokenizer)\n",
    "\n",
    "        res_emb.extend(emb)\n",
    "        res_pos.extend(pos+pos_len)\n",
    "        pos_len += pos[-1] + 1\n",
    "        partial_masks.append(mask)\n",
    "\n",
    "    return np.array(res_emb), np.array(res_pos), sum_masks(len(res_emb), partial_masks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_atomizer(tokenizer):\n",
    "    \"\"\"\n",
    "    {\n",
    "        \"token_id\":{\n",
    "            \"embedding\":np.array(int), \n",
    "            \"pe\":np.array(int), \n",
    "            \"mask\":np.array(np.array(union(0, 1)))\n",
    "        }\n",
    "    }\n",
    "    \"\"\"\n",
    "    tokenizex_atomizer = {}\n",
    "    vocab = {v:k for k, v in tokenizer.get_vocab().items()}\n",
    "    all_tok = vocab.keys()\n",
    "\n",
    "    for t in all_tok:\n",
    "        e, p, m = process_token(t, tokenizer)\n",
    "        tokenizex_atomizer[t] = (e, p, m)\n",
    "        \n",
    "    return tokenizex_atomizer\n",
    "        \n",
    "def embedding_mask_atomizer(txt, tokenizer, tokenizex_atomizer):\n",
    "    tokens = tokenizer.encode(txt)\n",
    "    partial_masks = []\n",
    "\n",
    "    res_emb = []\n",
    "    res_pos = []\n",
    "    pos_len = 0\n",
    "    \n",
    "    for t in tokens:\n",
    "        emb, pos, mask = tokenizex_atomizer[t]\n",
    "        res_emb.extend(emb)\n",
    "        res_pos.extend(pos+pos_len)\n",
    "        pos_len += pos[-1] + 1\n",
    "        partial_masks.append(mask)   \n",
    "    \n",
    "    full_mask = sum_masks(len(res_emb), partial_masks)\n",
    "    return np.array(res_emb), np.array(res_pos), full_mask\n",
    "\n",
    "def time_valid_tokenization(txt_data, tokenizer, def_tokenizer, C, lbl_val = False):\n",
    "    st = 0\n",
    "    for i in range(C):\n",
    "        txti = def_tokenizer.decode(def_tokenizer.encode(get_imput(txt_data, 512)))\n",
    "        t1 = time()\n",
    "        emb, pos, mask = tokenizer(txti)\n",
    "        st += time()-t1\n",
    "\n",
    "        if lbl_val: \n",
    "            lemb = tokenize_letters(txti, def_tokenizer)\n",
    "            if len(lemb) != len(emb):\n",
    "                print(txti)\n",
    "            assert len(lemb) == len(emb)\n",
    "\n",
    "            comp = pos[mask[-1].astype(bool)]\n",
    "            for a, b in zip(comp.astype(int), list(range(len(comp)))):\n",
    "                assert a == b\n",
    "\n",
    "    return st\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# atomizer = create_atomizer(tokenizer)\n",
    "vocab = tokenizer.get_vocab()\n",
    "vocabi = {v:k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_pos(poses):\n",
    "    con = [poses[0]]\n",
    "    for i in range(1, len(poses)):\n",
    "        con.append(poses[i]+con[i-1][-1])\n",
    "    return np.concatenate(con)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"wikipediawolna\"\n",
    "create_mask_embedding(txt, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize_atoms(txt, tokenizer):\n",
    "    atxt = \"\".join(tokenizer.tokenize(txt))\n",
    "    atxt = list(atxt)\n",
    "    eatxt = tokenizer.encode(atxt)\n",
    "    return eatxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def past_tokens_gather(atokens, tokenizer):\n",
    "    past_emb_pos = []\n",
    "    for i in range(1, len(atokens)+1):\n",
    "        partial = atokens[:i]\n",
    "        pdec = tokenizer.decode(partial)\n",
    "        penc = tokenizer.encode(pdec)\n",
    "        past_emb_pos.append(penc)\n",
    "    return past_emb_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Tworzenie macierzy 5x5 z jedynkami na przekątnej\n",
    "matrix = np.eye(5)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb_pos_mask(emb_pos):\n",
    "    res = []\n",
    "    mask = np.eye(len(emb_pos))\n",
    "    for embt, m in zip(emb_pos, mask):\n",
    "        res.append(embt[-1])\n",
    "        for ie in embt[:-1]:\n",
    "            is_added = True if res[:-1] == [] else False\n",
    "            for j, je in enumerate(res[:-1]):\n",
    "                if ie == je:\n",
    "                    m[j] = 1\n",
    "                    is_added = True\n",
    "                    break\n",
    "            assert is_added #dev\n",
    "    return np.array(res)[:,0], np.array(res)[:,1], mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tenc: [86, 72, 74, 72, 79, 68, 67, 72, 64, 86, 78, 75, 77, 64]\n",
      "dec: wikipediawolna\n"
     ]
    }
   ],
   "source": [
    "txt = \"wikipediawolna\"\n",
    "\n",
    "tenc = tokenize_atoms(txt, tokenizer)\n",
    "print(f\"tenc: {tenc}\")\n",
    "dec = tokenizer.decode(tenc)\n",
    "print(f\"dec: {dec}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(86, 0)], [(37686, 0)], [(20763, 0)], [(15466, 0)], [(20763, 0), (541, 1)], [(20763, 0), (3757, 1)], [(20763, 0), (46647, 1)], [(20763, 0), (541, 1), (13740, 2)], [(31266, 0)], [(31266, 0), (86, 1)], [(31266, 0), (21638, 1)], [(31266, 0), (86, 1), (349, 2)], [(31266, 0), (86, 1), (10875, 2)], [(31266, 0), (86, 1), (349, 2), (2616, 3)]]\n"
     ]
    }
   ],
   "source": [
    "res = past_tokens_gather(tenc, tokenizer)\n",
    "g = [list(zip(e, np.arange(len(e)))) for e in res]\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[86],\n",
       " [37686],\n",
       " [20763],\n",
       " [15466],\n",
       " [20763, 541],\n",
       " [20763, 3757],\n",
       " [20763, 46647],\n",
       " [20763, 541, 13740],\n",
       " [31266],\n",
       " [31266, 86],\n",
       " [31266, 21638],\n",
       " [31266, 86, 349],\n",
       " [31266, 86, 10875],\n",
       " [31266, 86, 349, 2616]]"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "e, p, m = create_emb_pos_mask(g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = []\n",
    "for r in m:\n",
    "    l.append(e[r.astype(bool)].tolist())\n",
    "l == res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partial: [86]\n",
      "pdec: ;w;\n",
      "penc: [86]\n",
      "partial: [86, 72]\n",
      "pdec: ;wi;\n",
      "penc: [37686]\n",
      "partial: [86, 72, 74]\n",
      "pdec: ;wik;\n",
      "penc: [20763]\n",
      "partial: [86, 72, 74, 72]\n",
      "pdec: ;wiki;\n",
      "penc: [15466]\n",
      "partial: [86, 72, 74, 72, 79]\n",
      "pdec: ;wikip;\n",
      "penc: [20763, 541]\n",
      "partial: [86, 72, 74, 72, 79, 68]\n",
      "pdec: ;wikipe;\n",
      "penc: [20763, 3757]\n",
      "partial: [86, 72, 74, 72, 79, 68, 67]\n",
      "pdec: ;wikiped;\n",
      "penc: [20763, 46647]\n",
      "partial: [86, 72, 74, 72, 79, 68, 67, 72]\n",
      "pdec: ;wikipedi;\n",
      "penc: [20763, 541, 13740]\n",
      "partial: [86, 72, 74, 72, 79, 68, 67, 72, 64]\n",
      "pdec: ;wikipedia;\n",
      "penc: [31266]\n",
      "partial: [86, 72, 74, 72, 79, 68, 67, 72, 64, 86]\n",
      "pdec: ;wikipediaw;\n",
      "penc: [31266, 86]\n",
      "partial: [86, 72, 74, 72, 79, 68, 67, 72, 64, 86, 78]\n",
      "pdec: ;wikipediawo;\n",
      "penc: [31266, 21638]\n",
      "partial: [86, 72, 74, 72, 79, 68, 67, 72, 64, 86, 78, 75]\n",
      "pdec: ;wikipediawol;\n",
      "penc: [31266, 86, 349]\n",
      "partial: [86, 72, 74, 72, 79, 68, 67, 72, 64, 86, 78, 75, 77]\n",
      "pdec: ;wikipediawoln;\n",
      "penc: [31266, 86, 10875]\n",
      "partial: [86, 72, 74, 72, 79, 68, 67, 72, 64, 86, 78, 75, 77, 64]\n",
      "pdec: ;wikipediawolna;\n",
      "penc: [31266, 86, 349, 2616]\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, len(tenc)+1):\n",
    "    partial = tenc[:i]\n",
    "    print(f\"partial: {partial}\")\n",
    "    pdec = tokenizer.decode(partial)\n",
    "    print(f\"pdec: ;{pdec};\")\n",
    "    penc = tokenizer.encode(pdec)\n",
    "    print(f\"penc: {penc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(txt, emb, pos, mask, tokenizer):\n",
    "    dec = tokenizer.decode(emb[mask[-1].astype(bool)])\n",
    "    assert dec == txt\n",
    "\n",
    "    lemb = tokenize_atoms(txt, tokenizer)\n",
    "    assert len(lemb) == len(emb)\n",
    "\n",
    "    comp = pos[mask[-1].astype(bool)]\n",
    "    for a, b in zip(comp.astype(int), list(range(len(comp)))):\n",
    "        assert a == b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"\"\" ósiem\"\"\"\n",
    "emb, pos, mask = encode_em(txt, tokenizer)\n",
    "validate(txt, emb, pos, mask, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \" ósiem\"\n",
    "\n",
    "tokens = tokenizer.tokenize(txt)\n",
    "res_emb = []\n",
    "res_pos = []\n",
    "partial_masks = []\n",
    "pos_len = 0\n",
    "\n",
    "for t in tokens:\n",
    "    emb, pos, mask = process_token(t, tokenizer)\n",
    "\n",
    "    res_emb.extend(emb)\n",
    "    res_pos.extend(pos+pos_len)\n",
    "    pos_len += pos[-1] + 1\n",
    "    partial_masks.append(mask)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1  = \"elo źle żal ósemka no i śląska\"\n",
    "t3 = \"example text to tokenize and, żargon ołówka źle wpływa detokenize, ślóza.\"\n",
    "t1  = \"elo źle żaló\"\n",
    "t3 = \"example.\"\n",
    "e2  = np.array([69])\n",
    "p2  = np.array([0])\n",
    "m2  = np.array([1])\n",
    "t2 = tokenizer.decode(e2)\n",
    "\n",
    "e1, p1, m1 = encode_em(t1, tokenizer)\n",
    "e3, p3, m3 = encode_em(t3, tokenizer)\n",
    "txt = t1+t2+t3\n",
    "txt\n",
    "\n",
    "emb = np.concatenate([e1,e2,e3])\n",
    "pos = sum_pos([p1,p2,p3])\n",
    "mlen = m1.shape[0]+m2.shape[0]+m3.shape[0]\n",
    "mask = sum_masks(mlen, [m1, m2, m3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faulty examples\n",
    "' ósiem'\n",
    "\"ó\"\n",
    "\"\"\" ›\"\"\"\n",
    "''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def text_emb_pos(txt, tokenizer):\n",
    "    emb_pos = []\n",
    "\n",
    "    letter_split = False\n",
    "    for l in txt:\n",
    "        t = tokenizer.encode(l)\n",
    "        if len(t) > 1:\n",
    "            letter_split = True\n",
    "    \n",
    "    if letter_split:\n",
    "        for l in txt:\n",
    "            et = tokenizer.encode(l)\n",
    "            emb_pos.append(list(zip(et, list(range(len(et))))))\n",
    "    else:\n",
    "        for i in range(1, len(txt)+1):\n",
    "            et = tokenizer.encode(txt[:i])\n",
    "            emb_pos.append(list(zip(et, list(range(len(et))))))\n",
    "\n",
    "    return emb_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in atomizer.keys():\n",
    "    txt = tokenizer.decode(t)\n",
    "    emb = tokenize_letters(txt, tokenizer)\n",
    "    a_emb = atomizer[t][0]\n",
    "    assert len(emb) == len(a_emb)\n",
    "    if len(emb) != len(a_emb):\n",
    "        print(len(emb), len(a_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_letters(txt, tokenizer):\n",
    "    emb = []\n",
    "    for l in txt:\n",
    "        emb.extend(tokenizer.encode(l))\n",
    "    return emb\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "randomllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
