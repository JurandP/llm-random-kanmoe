{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from functools import cache\n",
    "\n",
    "\n",
    "def _text_sub_emb_pos(txt, tokenizer):\n",
    "    emb_pos = []\n",
    "    for i in range(1, len(txt)+1):\n",
    "        et = tokenizer.encode(txt[:i])\n",
    "        emb_pos.append(list(zip(et, list(range(len(et))))))\n",
    "    return emb_pos\n",
    "\n",
    "def _create_subs_emb_mask(emb_pos):\n",
    "    total_len = len(emb_pos)\n",
    "    emb = []\n",
    "    mask = np.zeros([total_len, total_len])\n",
    "    for embt, m in zip(emb_pos, mask):\n",
    "        emb.append(embt[-1])\n",
    "        m[len(emb)-1] = 1\n",
    "        for ie in embt[:-1]:\n",
    "            for j, je in enumerate(emb[:-1]):\n",
    "                if je == ie:\n",
    "                    m[j] = 1\n",
    "                    break\n",
    "    return np.array(emb), mask\n",
    "\n",
    "def _create_emb_pos_mask(txt, tokenizer):\n",
    "    emb_pos = _text_sub_emb_pos(txt, tokenizer)\n",
    "    res, mask = _create_subs_emb_mask(emb_pos)    \n",
    "    return res[:, 0], res[:, 1], mask\n",
    "\n",
    "def add_masks(o_mask_size, masks):\n",
    "    iy = 0\n",
    "    ix = 0\n",
    "    o_mask = np.zeros((o_mask_size, o_mask_size))\n",
    "\n",
    "    for m in masks:\n",
    "        ms = len(m)\n",
    "        o_mask[iy:iy+ms, ix:ix+ms] += m\n",
    "        o_mask[iy+ms:, ix:ix+ms] = m[-1]\n",
    "        iy+=ms\n",
    "        ix+=ms\n",
    "    return o_mask\n",
    "\n",
    "@cache\n",
    "def _process_token(t, tokenizer):\n",
    "    td = tokenizer.decode(t)\n",
    "    if len(td)==1 or \"ï¿½\" in td:\n",
    "        emb = np.array([t]) \n",
    "        pos = np.zeros((1))\n",
    "        mask = np.ones((1,1))\n",
    "    else:\n",
    "        emb, pos, mask = _create_emb_pos_mask(td, tokenizer)\n",
    "    return emb, pos, mask\n",
    "\n",
    "def create_token_atomizer(tokenizer) -> dict[int, tuple[np.array, np.array, np.array]]:\n",
    "    \"\"\"\n",
    "    Based on given tokenizer returns atomizer for its tokens (cached alternative, _process_token for every token),\n",
    "    {\n",
    "        token_id:{\n",
    "            embedding, \n",
    "            positiolan_encoding_positions, \n",
    "            attention_mask\n",
    "        }\n",
    "    }\n",
    "    contains each token informations: its atom-tokens, positional encoding positions and attantion mask <- all in tokenizex processing format \n",
    "\n",
    "    :param _type_ tokenizer: _description_\n",
    "    :return dict[int, tuple[np.array, np.array, np.array]]: aka. atomizer, \n",
    "    \"\"\"\n",
    "    tokenizex_atomizer = {}\n",
    "    vocab = {v:k for k, v in tokenizer.get_vocab().items()}\n",
    "    all_tok = vocab.keys()\n",
    "\n",
    "    for t in all_tok:\n",
    "        e, p, m = _process_token(t, tokenizer)\n",
    "        tokenizex_atomizer[t] = (e, p, m)\n",
    "    \n",
    "    return tokenizex_atomizer\n",
    "        \n",
    "\n",
    "\n",
    "def embedding_mask(txt, tokenizer, tokenizex_atomizer): #v2\n",
    "    global st\n",
    "    tokens = tokenizer.encode(txt)\n",
    "    partial_masks = []\n",
    "\n",
    "    res_emb = []\n",
    "    res_pos = []\n",
    "    pos_len = 0\n",
    "\n",
    "    \n",
    "    for t in tokens:\n",
    "        emb, pos, mask = tokenizex_atomizer[t]\n",
    "        res_emb.extend(emb)\n",
    "        res_pos.extend(pos+pos_len)\n",
    "        pos_len += pos[-1] + 1\n",
    "        partial_masks.append(mask)\n",
    "                \n",
    "    \n",
    "    full_mask = add_masks(len(res_emb), partial_masks)\n",
    "    \n",
    "    return np.array(res_emb), res_pos, full_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "randomllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
