md5_parent_hash: 7a0330388646a9bfbcbee67008f9538a
parent: research/conditional/train/configs/baselines/gpt/dense/medium.yaml
n_gpus: 1
time: "20:00:00"
hf_datasets_cache: /raid/NFS_SHARE/home/maciej.pioro/datasets

params:
  name: "mamba_tune"
  mixed_precision: true
  mixed_precision_dtype: bfloat16
  flash_attention: true

  dataset_type: c4
  # fsdp
  fsdp_enabled: true
  fsdp_modules_to_wrap: "EmbeddingLayer,PredictionHead,TransformerBlock"
  activation_checkpointing_modules: "EmbeddingLayer,PredictionHead,TransformerBlock"
  fsdp_selective_precision_modules: "AttentionMechanism,ExpertGating,RoPE,TokenChoiceRouter"

  grad_clip: 0.5
  weight_decay: 0.1

  scheduler: "cosine"
  decoding_interval: 0
  save_weights_interval: 0
  loss_checkpoint_chungs: 0
  gradient_accumulation_steps: 1

  lr_warmup_steps: 1500
  n_steps: 150_000
  final_lr_step: 150_000

  batch_size: 64
  cutoff: 1024

  init_scale: 0.1
  init_type: truncated_normal

  # grid
  # ^learning_rate: [2e-4, 5e-4]
  ^learning_rate: [5e-4]

  # mamba / moe etc
  n_blocks: 8
  block_modules: ["attention", "feedforward"]
  attention_mode: "rope"
  no_positional_embedding: false
  ff_mode: vanilla
  ^n_experts: [16]
  dff: 2048
  expert_size: 2048
  capacity_factor: 1.0
  load_balancing_loss_weight: 0.01
  ^expert_cycling_dropped_fraction: [0.0]

  # debug
  # ^expert_cycling_dropped_fraction: [0.5]
  # ^n_experts: [16]
  # fsdp_enabled: false

  # wandb
  project_name: cycling
  use_neptune: false
  use_wandb: true

  # debug
  # dataset_type: wikibook
  # use_dummy_dataset: true
  # batch_size: 2
  # fsdp_enabled: false
  # n_blocks: 1
