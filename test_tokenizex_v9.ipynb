{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast, GPT2Tokenizer\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from random import random\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from embedding_mask import sum_masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maciej/anaconda3/envs/randomllm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "vocab = tokenizer.get_vocab()\n",
    "vocabi = {v:k for k, v in vocab.items()}\n",
    "SRC = \"asd_pol.txt\"\n",
    "with open(SRC, \"r\") as f:\n",
    "    all_txt = f.read()\n",
    "def get_imput(txt, length=512):\n",
    "    bi = int(random()*(len(txt)-1000))\n",
    "    return txt[bi:bi+length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from functools import cache\n",
    "\n",
    "\n",
    "def tokenize_atoms(txt, tokenizer):\n",
    "    atxt = \"\".join(tokenizer.tokenize(txt))\n",
    "    atxt = list(atxt)\n",
    "    eatxt = tokenizer.encode(atxt)\n",
    "    return eatxt\n",
    "\n",
    "c = 0\n",
    "\n",
    "def lazy_past_tokens_gather(atokens, tokenizer): #dev no dobra, może tak być dla tokenów kłopotliwych, ale dla reszty już nie powinno\n",
    "    global c #dev\n",
    "    c+=1 #dev\n",
    "    past_emb_pos = []\n",
    "    for i in range(1, len(atokens)+1):\n",
    "        past_emb_pos.append(atokens[:i])\n",
    "    return past_emb_pos\n",
    "\n",
    "def past_tokens_gather(atokens, tokenizer):\n",
    "    past_emb_pos = []\n",
    "    for i in range(1, len(atokens)+1):\n",
    "        pdec = tokenizer.decode(atokens[:i])\n",
    "        if '�' in pdec:\n",
    "            return lazy_past_tokens_gather(atokens, tokenizer)\n",
    "        penc = tokenizer.encode(pdec)\n",
    "        past_emb_pos.append(penc)\n",
    "    return past_emb_pos\n",
    "\n",
    "def create_emb_pos_mask(emb_pos):\n",
    "    res = []\n",
    "    # mask = np.eye(len(emb_pos)).astype(bool)\n",
    "    mask = np.eye(len(emb_pos))\n",
    "    for embt, m in zip(emb_pos, mask):\n",
    "        res.append(embt[-1])\n",
    "        for ie in embt[:-1]:\n",
    "            is_added = True if res[:-1] == [] else False\n",
    "            for j, je in enumerate(res[:-1]):\n",
    "                if ie == je:\n",
    "                    m[j] = 1\n",
    "                    is_added = True\n",
    "                    break\n",
    "            if not is_added:\n",
    "                raise Exception(\"target len != input len\")\n",
    "    return np.array(res)[:,0], np.array(res)[:,1], mask\n",
    "\n",
    "# @cache\n",
    "def tokenizex_encode_full(txt, tokenizer):\n",
    "    tenc = tokenize_atoms(txt, tokenizer)\n",
    "    res = past_tokens_gather(tenc, tokenizer)\n",
    "    g = [list(zip(e, np.arange(len(e)))) for e in res]\n",
    "    e, p, m = create_emb_pos_mask(g)\n",
    "    return e, p, m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizex_encode(txt, tokenizer, atomizer):\n",
    "    tokens = tokenizer.encode(txt)\n",
    "\n",
    "    res_emb = []\n",
    "    res_pos = []\n",
    "    partial_masks = []\n",
    "    pos_len = 0\n",
    "    \n",
    "    for t in tokens:\n",
    "        emb, pos, mask = atomizer[t]\n",
    "        res_emb.extend(emb)\n",
    "        res_pos.extend(pos+pos_len)\n",
    "        pos_len += pos[-1] + 1\n",
    "        partial_masks.append(mask)   \n",
    "    \n",
    "    full_mask = sum_masks(len(res_emb), partial_masks)\n",
    "    return np.array(res_emb), np.array(res_pos), full_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def remove_whitespace_before_dots_p(text):\n",
    "    text = tokenizer.decode(tokenizer.encode(text))\n",
    "    text = re.sub(r'\\s+\\.', '.', text)\n",
    "    text = re.sub(r\"\\s+\\'\", \"'\", text)\n",
    "    text = re.sub(r'\\s+\\,', ',', text)\n",
    "    text = re.sub(r'\\s+\\!', '!', text)\n",
    "    text = re.sub(r'\\s+\\?', '?', text)\n",
    "    text = re.sub(r'\\n+\\n', '\\n', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(txt, emb, pos, mask, tokenizer):\n",
    "    txt = remove_whitespace_before_dots_p(txt)\n",
    "    # dec = tokenizer.decode(emb[mask[-1].astype(bool)])\n",
    "    # assert dec == txt # dev transformer has no order, token-po out of order\n",
    "\n",
    "    lemb = tokenize_atoms(txt, tokenizer)\n",
    "    assert len(lemb) == len(emb)\n",
    "\n",
    "    # comp = pos[mask[-1].astype(bool)]\n",
    "    # for a, b in zip(comp.astype(int), list(range(len(comp)))):\n",
    "    #     assert a == b # dev transformer has no order, token-po out of order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizex_w_encode(txt, tokenizer):\n",
    "    res_emb = []\n",
    "    res_pos = []\n",
    "    partial_masks = []\n",
    "    pos_len = 0\n",
    "    \n",
    "    txtw = [\" \" + e for e in txt.split(\" \")]\n",
    "    if len(txtw[0]) == 1:\n",
    "        del txtw[0]\n",
    "    else:\n",
    "        txtw[0] = txtw[0][1:]\n",
    "    \n",
    "    for w in txtw:\n",
    "        emb, pos, mask = tokenizex_encode_full(w, tokenizer)\n",
    "        res_emb.extend(emb)\n",
    "        res_pos.extend(pos+pos_len)\n",
    "        pos_len += pos[-1] + 1\n",
    "        partial_masks.append(mask)   \n",
    "    full_mask = sum_masks(len(res_emb), partial_masks)\n",
    "    return np.array(res_emb), np.array(res_pos), full_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cial Maurice Rossel reported that conditions there were \"almost normal\".\\n2014 – Under the terms of UN Security C'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = \" return np.array(res_emb), np.array(res_pos), full_mask\"\n",
    "txt = \"\"\"  𡨸 𡨸żóąs\"\"\"\n",
    "txt = \"\"\"cial Maurice Rossel reported that conditions there were \"almost normal\".\n",
    "2014 – Under the terms of UN Security C\"\"\"\n",
    "txt = tokenizer.prepare_for_tokenization(txt)[0]\n",
    "txt = remove_whitespace_before_dots_p(txt)\n",
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# txt = errorous\n",
    "txt = \"\"\"\\n\\nY\"\"\"\n",
    "txt = '\\n\\nf\\ngh\\n\\n\\n\\nSta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = remove_whitespace_before_dots_p(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[220], [299], [299, 6], [77, 470]]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[220, 77, 6, 83]\n",
    "[[220], [299], [299, 6], [77, 470]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"n't\""
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([77, 470])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[77, 6, 83]\n",
      "[[77], [77, 6], [77, 470]]\n",
      "[ 77   6 470]\n",
      "[[1. 0. 0.]\n",
      " [1. 1. 0.]\n",
      " [1. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "txt = \" n't\"\n",
    "txt = remove_whitespace_before_dots_p(txt)\n",
    "tenc = tokenize_atoms(txt, tokenizer)\n",
    "print(tenc)\n",
    "res = past_tokens_gather(tenc, tokenizer)\n",
    "print(res)\n",
    "g = [list(zip(e, np.arange(len(e)))) for e in res]\n",
    "e, p, m = create_emb_pos_mask(g)\n",
    "print(e)\n",
    "print(m)\n",
    "validate(txt, e, p, m, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "e, p, m = tokenizex_encode_full(txt, tokenizer)\n",
    "validate(txt, e, p, m, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee, pp, mm = tokenizex_w_encode(txt, tokenizer)\n",
    "# print(ee)\n",
    "# print(mm)\n",
    "validate(txt, ee, pp, mm, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_def(txt, tokenizer):\n",
    "    return tokenizer.encode(txt), None, None\n",
    "\n",
    "def tokenizer_my1(txt, tokenizer):\n",
    "    tenc = tokenize_atoms(txt, tokenizer)\n",
    "    res = past_tokens_gather(tenc, tokenizer)\n",
    "    g = [list(zip(e, np.arange(len(e)))) for e in res]\n",
    "    e, p, m = create_emb_pos_mask(g)\n",
    "    return e, p, m\n",
    "\n",
    "def tokenizer_my2(txt, tokenizer):\n",
    "    e, p, m= tokenizex_w_encode(txt, tokenizer)\n",
    "    return e, p, m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "errorous = \"\"\n",
    "def time_valid_tokenization(txt_data, tokenizer, def_tokenizer, C, lbl_val = False):\n",
    "    st = 0\n",
    "    global errorous\n",
    "    for i in range(C):\n",
    "        txt = get_imput(txt_data, 512)\n",
    "        # txti = def_tokenizer.decode(def_tokenizer.encode(get_imput(txt_data, 112)))\n",
    "        txt = def_tokenizer.prepare_for_tokenization(txt)[0]\n",
    "        txt = remove_whitespace_before_dots_p(txt)\n",
    "        t1 = time()\n",
    "        errorous = txt\n",
    "        emb, pos, mask = tokenizer(txt, def_tokenizer)\n",
    "        st += time()-t1\n",
    "\n",
    "        if lbl_val: \n",
    "            validate(txt, emb, pos, mask, def_tokenizer)\n",
    "\n",
    "    return st\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \" n't\"\n",
    "txt = remove_whitespace_before_dots_p(txt)\n",
    "emb, pos, mask = tokenizer_my2(txt, tokenizer)\n",
    "validate(txt, emb, pos, mask, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "default = time_valid_tokenization(all_txt, tokenizer_def, tokenizer, 500, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my2 = time_valid_tokenization(all_txt, tokenizer_my2, tokenizer, 500, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from research.tokenizex.model.tokenizer import TokenizexTokenizer\n",
    "tokenizex = TokenizexTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizex_fin(txt,_):\n",
    "    return tokenizex.text_to_ids_pos_mask(txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "mytx = time_valid_tokenization(all_txt, tokenizex_fin, tokenizer, 50, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mytx/default ratio 8.541665056435386\n"
     ]
    }
   ],
   "source": [
    "llm_examples = 128*500000\n",
    "\n",
    "print(f\"mytx/default ratio {mytx/default}\")\n",
    "# print(f\"my2/default ratio {my2/default}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_pos(poses):\n",
    "    res = []\n",
    "    res.extend(poses[0])\n",
    "    sum_slit = 0\n",
    "    for i in range(1, len(poses)):\n",
    "        sum_slit += poses[i-1][-1] + 1\n",
    "        res.extend(poses[i] + sum_slit)\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = \"example text to tok\\\\n\\n\\n\\n\\n\\n\\\\n\\n\\n\\\\n\\\\nn\\enize n\\nn\\and, żan\\n\\n\\rgon\\n\\n ołn\\wkn\\n\\n\\a źn\\e wpłn\\n\\\\n\\ywa detokenize, ślóza.\"\n",
    "t2 = \" ... ż eloó ziomelo\"\n",
    "t3 = \" n't\"\n",
    "\n",
    "e1, p1, m1 = tokenizex.text_to_ids_pos_mask(t1)\n",
    "e2, p2, m2 = tokenizex.text_to_ids_pos_mask(t2)\n",
    "e3, p3, m3 = tokenizex.text_to_ids_pos_mask(t3)\n",
    "txt = t1+t2+t3\n",
    "txt\n",
    "\n",
    "emb = np.concatenate([e1,e2,e3])\n",
    "pos = sum_pos([p1,p2,p3])\n",
    "mlen = m1.shape[0]+m2.shape[0]+m3.shape[0]\n",
    "mask = sum_masks(mlen, [m1, m2, m3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate(txt, emb, pos, mask, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod\n",
    "import random\n",
    "from typing import Optional\n",
    "\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class AbstractDataset:\n",
    "    def __init__(self, seed: Optional[int] = None):\n",
    "        self.set_rng(seed)\n",
    "\n",
    "    def set_rng(self, seed: Optional[int] = None):\n",
    "        np_rng = np.random.default_rng(seed)\n",
    "        py_rng = random.Random(seed)\n",
    "\n",
    "        self.np_rng = np_rng\n",
    "        self.py_rng = py_rng\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_document(self) -> str:\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    \n",
    "class WikiBookDataset(AbstractDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        seed: Optional[int] = None,\n",
    "        use_dummy_dataset: bool = False,\n",
    "        split: str = \"train\",\n",
    "    ):\n",
    "        super().__init__(seed=seed)\n",
    "        assert split in [\"train\", \"eval\"]\n",
    "        self.split = split\n",
    "\n",
    "        self.dataset_wiki = load_dataset(\n",
    "            \"wikipedia\", f\"20220301.{'simple' if use_dummy_dataset else 'en'}\"\n",
    "        )[\"train\"]\n",
    "        self.dataset_book = (\n",
    "            load_dataset(\"bookcorpus\")[\"train\"]\n",
    "            if not use_dummy_dataset\n",
    "            else self.dataset_wiki\n",
    "        )\n",
    "\n",
    "        self.bookcorpus_chance = len(self.dataset_book) / len(self.dataset_wiki)\n",
    "\n",
    "    def get_document(self) -> str:\n",
    "        selector = self.py_rng.random()\n",
    "        if selector < self.bookcorpus_chance:\n",
    "            return self._get_random_book_example()\n",
    "        else:\n",
    "            return self._get_random_wiki_example()\n",
    "\n",
    "    def _belongs_to_split(self, document_id: int) -> bool:\n",
    "        eval_percentage = 5\n",
    "\n",
    "        if self.split == \"train\":\n",
    "            return hash(document_id) % 100 >= eval_percentage\n",
    "        elif self.split == \"eval\":\n",
    "            return hash(document_id) % 100 < eval_percentage\n",
    "        else:\n",
    "            raise ValueError(\"split must be either 'train' or 'eval'\")\n",
    "\n",
    "    def _get_random_book_example(self) -> str:\n",
    "        doc_id = None\n",
    "        while doc_id is None or not self._belongs_to_split(doc_id):\n",
    "            doc_id = self.py_rng.randint(0, len(self.dataset_book) - 1)\n",
    "        document = self.dataset_book[doc_id]\n",
    "        return document[\"text\"]\n",
    "\n",
    "    def _get_random_wiki_example(self) -> str:\n",
    "        doc_id = None\n",
    "        while doc_id is None or not self._belongs_to_split(doc_id):\n",
    "            doc_id = self.py_rng.randint(0, len(self.dataset_book) - 1)\n",
    "        document = self.dataset_wiki[doc_id]\n",
    "        return document[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "errorous = \"\"\n",
    "def time_valid_tokenization(wb_data, tokenizer, def_tokenizer, C, lbl_val = False):\n",
    "    st = 0\n",
    "    global errorous\n",
    "    for i in range(C):\n",
    "        txt = wb_data.get_document()[:512]\n",
    "        # txti = def_tokenizer.decode(def_tokenizer.encode(get_imput(txt_data, 112)))\n",
    "        txt = def_tokenizer.prepare_for_tokenization(txt)[0]\n",
    "        txt = remove_whitespace_before_dots_p(txt)\n",
    "        errorous = txt\n",
    "        t1 = time()\n",
    "        emb, pos, mask = tokenizer(txt, def_tokenizer)\n",
    "        st += time()-t1\n",
    "\n",
    "        if lbl_val: \n",
    "            validate(txt, emb, pos, mask, def_tokenizer)\n",
    "\n",
    "    return st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_data = WikiBookDataset(2137, False, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default = time_valid_tokenization(wb_data, tokenizer_def, tokenizer, C, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my = time_valid_tokenization(wb_data, tokenizex_fin, tokenizer, C, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3123.239609479904"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio my/default: 49.925213790288886\n"
     ]
    }
   ],
   "source": [
    "print(f\"ratio my/default: {my/default}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'while they buried our friend, i watched a glossy emerald beetle, carapace age-pocked and mossy, fend off an attack by a dozen fuzzy ants, their red thorax glands releasing tiny jets of bubbly white poison.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errorous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# txt = errorous\n",
    "txt = \" n't\"\n",
    "e, p, m = tokenizex.text_to_ids_pos_mask(txt)\n",
    "validate(txt, e, p, m, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[220, 77, 6, 83]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_atoms(txt, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 77,   6, 470])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 77, 470])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e[m[-1].astype(bool)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p[m[-1].astype(bool)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[299, 470]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"n't\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(e[m[-1].astype(bool)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ġn', \"'t\"]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"n't\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = \" n't\"\n",
    "d = tokenizer.encode(txt)\n",
    "tokenizer.decode(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "; ! ;\n",
      ";! ;\n",
      "\n",
      "; ' ;\n",
      ";';\n",
      "\n",
      "; , ;\n",
      ";, ;\n",
      "\n",
      "; . ;\n",
      ";. ;\n",
      "\n",
      "; ? ;\n",
      ";? ;\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "# Iterate over all possible Unicode code points\n",
    "for code_point in range(1118481):  # 0x110000 is 0x10FFFF + 1\n",
    "    try:\n",
    "        character = chr(code_point)\n",
    "        # Filter out non-printable characters by their category\n",
    "        if unicodedata.category(character)[0] != 'C':\n",
    "            \n",
    "            txt = f\" {character} \"\n",
    "            d = tokenizer.encode(txt)\n",
    "            if txt != tokenizer.decode(d):\n",
    "                print(f\";{txt};\")\n",
    "                print(f\";{tokenizer.decode(d)};\")\n",
    "                print()\n",
    "    except ValueError:\n",
    "        # Skip invalid code points\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2]),\n",
       " array([0, 1, 2]),\n",
       " array([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sl = 3\n",
    "e = np.arange(7)\n",
    "m = np.ones((7,7))\n",
    "p = np.arange(7)\n",
    "surpass = len(e) - sl\n",
    "if surpass > 0:\n",
    "    e = e[:-surpass]\n",
    "    p = p[:-surpass]\n",
    "    m = m[:-surpass, :-surpass]\n",
    "\n",
    "e, p, m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "randomllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
